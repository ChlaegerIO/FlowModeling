{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8f0b7e",
   "metadata": {},
   "source": [
    "PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b0ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print progress, possibly faster than progress bar\n",
    "def print_progress(epoch, batch_id, accuracy, loss):\n",
    "    progress = '='* int((10. * batch_id / len(train_data)))\n",
    "    progress += '>'\n",
    "    if batch_id == 1:\n",
    "        print('Train Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='')\n",
    "    else:\n",
    "        print('\\rTrain Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f332b95",
   "metadata": {},
   "source": [
    "Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83753ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing odd number 9: 100%|███████████████████████████████████| 10/10 [00:05<00:00,  1.97carrots/s, divisors=[1, 3, 9]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "def get_divisors(n):\n",
    "    divisors = []\n",
    "    for m in range(1, n+1):\n",
    "        if n % m == 0:\n",
    "            divisors.append(m)\n",
    "    return divisors\n",
    "\n",
    "iterations = 10\n",
    "with trange(iterations, unit=\"carrots\") as pbar:\n",
    "    for i in pbar:\n",
    "        sleep(0.5)\n",
    "        if i % 2:\n",
    "            pbar.set_description(f\"Testing odd number {i}\")\n",
    "        else:\n",
    "            pbar.set_description(f\"Testing even number {i}\")\n",
    "        pbar.set_postfix(divisors=get_divisors(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8163b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9733, 1.6265, 2.4631],\n",
      "        [1.7833, 0.7271, 0.1025],\n",
      "        [0.8165, 1.9885, 2.2756],\n",
      "        [0.1511, 1.4171, 0.6514],\n",
      "        [1.5482, 2.5004, 0.0784]])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'catdog/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20048/1410374020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mtarget_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'catdog/train/'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# Tutorial: https://www.youtube.com/watch?v=GBzojftwfGQ&list=PLNmsVeXQZj7rx55Mai21reZtd_8m-qe27&index=5&ab_channel=TheMorpheusTutorials\n",
    "# 1. Tensor\n",
    "x = torch.Tensor(5,3)   # numpy arrays werden durch Tensoren ersetzt\n",
    "x[:,:] = 1\n",
    "y = torch.randn(5,3)    # random tenseor\n",
    "#print(y)\n",
    "xy = torch.add(x,y)\n",
    "print(xy)\n",
    "\n",
    "# 2. Für Grafik Karte\n",
    "#if torch.cuda.is_available():\n",
    "#    x = x.cuda()       # RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable \n",
    "#    y = y.cude()\n",
    "#print(x+y)\n",
    "\n",
    "# 3. Daten laden \n",
    "kwargs = {}\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)\n",
    "# Bilder laden und verarbeiten\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from os import listdir\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])    # normalize around mean with sigma (std)\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), normalize])\n",
    "train_data_list = []\n",
    "train_data = []\n",
    "target_label = []\n",
    "for f in listdir('catdog/train/'):\n",
    "    img = Image.open('catdog/train/' + f)\n",
    "    img_tensor = transforms(img)\n",
    "    img_tensor.unsqueeze_(0)   # save directly in img_tensor with _ at the end (1,3,256,256)\n",
    "    # label\n",
    "    isCat = 1 if 'cat' in f else 0\n",
    "    isDog = 1 if 'dog' in f else 0\n",
    "    target = [isCat, isDog]\n",
    "    target_label.append(target)\n",
    "    if len(train_data_list) >= 64:\n",
    "        train_data.append(torch.stack(train_data_list), target_label)\n",
    "        train_data_list = []\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591cf2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 4. Erste eigene Neuronale Netzwerk\n",
    "class MyNet(nn.Module): # Vererbungslehre\n",
    "    def __init__(self):              # initialize network\n",
    "        super(MyNet,self).__init__()\n",
    "        self.lin1 = nn.Linear(10,10)\n",
    "        self.lin2 = nn.Linear(10,10)\n",
    "    \n",
    "    def forward(self, x):            # forward pass function\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size() [1:]         # ohne batch dimension\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num\n",
    "    \n",
    "net = MyNet()\n",
    "#net = net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e629a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4419, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1214, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1770e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7166e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4889e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4567e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5887e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8590e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2454e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7293e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2954e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9305e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6237e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3656e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1485e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.6599e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1247e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8336e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7478e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8344e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0662e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4201e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8768e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4197e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0352e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7119e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4399e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2112e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0188e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5691e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2079e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0628e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0997e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2896e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6081e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0348e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5528e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1474e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8063e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5193e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2780e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0750e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0434e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6051e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3985e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3819e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5267e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.0230, -0.0031, -0.0313, -0.0392,  1.1351, -0.0732, -0.0094,  0.0220,\n",
      "         1.0155,  1.0232], grad_fn=<AddBackward0>)\n",
      "tensor([[ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 5. main training loop with input\n",
    "for i in range(100):\n",
    "    x = [0,1,1,1,0,1,1,1,0,0]\n",
    "    input = Variable(torch.Tensor([x for _ in range(10)]))    # 10x10 input variable, weil man ein batch von 10 daraus erstellt\n",
    "    out = net(input)                      # input durch Netz durchlaufen lassen\n",
    "\n",
    "    # labels what to expect\n",
    "    x = [1,0,0,0,1,0,0,0,1,1]\n",
    "    target = Variable(torch.Tensor([x for _ in range(10)]))\n",
    "    # loss\n",
    "    criterion = nn.MSELoss()              # Fehlerfunktionen: optimieren (softwmax, MSE, ...)\n",
    "    loss = criterion(out, target)\n",
    "    print(loss)\n",
    "\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1,)            # optimizer ausprobieren SGD, Adam, ...\n",
    "    optimizer.step()\n",
    "    y\n",
    "    \n",
    "test_input = Variable(torch.Tensor([1,1,1,1,0,0,0,1,1,1]))\n",
    "test_out = net(test_input)\n",
    "print(test_out)  \n",
    "\n",
    "print(out)\n",
    "\n",
    "# 6. Speichern und Laden des Trainierten Netzes (pro Zyklus speichern, falls das Programm abstürtzt während dem trainieren)\n",
    "torch.save(net, 'data_figures_nn/MyNet.pt')\n",
    "if os.path.isfile('data_figures_nn/MyNet.pt'):\n",
    "    net_load = torch.load('data_figures_nn/MyNet.pt')\n",
    "\n",
    "print(net_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b82887",
   "metadata": {},
   "source": [
    "MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f6c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data_figures_nn\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3082,))\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████| 938/938 [00:55<00:00, 16.96batch/s, accuracy=45.3, loss=0.203]\n",
      "c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.0785710596010089\n",
      "accuracy:  97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████| 938/938 [00:55<00:00, 17.03batch/s, accuracy=45.3, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.08161891804933548\n",
      "accuracy:  97 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "kwargs = {}          # without GPU\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True}        # with GPU\n",
    "batch_size=64\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "print(len(train_data.dataset))\n",
    "\n",
    "test_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=False, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "print(train_data.dataset)\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        # definition of object layers\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)    # (in_channels: Eingangsdimension, out_channels: Ausgangsdimension)\n",
    "        # (kernel_size: Faltungsfiltergrösse, stride: #Schritte der Faltung (reduziert Dimension), padding: Leerer Rand dass Dimension weniger abnimmt, dilation: adding zeros between kernel, groups: , bias: added to output, padding_mode='zeros')\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        # (p=0.5: probability for dropout, inplace: if True input layer gets changed)\n",
    "        self.conv_dropout = nn.Dropout2d()            # damit das Netz nichts auswendig lernt\n",
    "        self.fc1 = nn.Linear(320, 60)\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # actual layer composition\n",
    "        #print('start:',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        # (kernel_size, stride, padding, dilation, return_indices: of max value if True, ceil_mode: ceil if True (aufrunden))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool1:', x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1:',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:',x.shape)\n",
    "        x = self.conv_dropout(x)\n",
    "        #print('dropout:',x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool2:',x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2:', x.shape)\n",
    "        x = x.view(-1,320)        # daten umformen\n",
    "        #print('view:', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print('relu3_fc1:', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print('fc2:', x.shape)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "if os.path.isfile('data_figures_nn/MyNet_MNIST.pt'):\n",
    "    net = torch.load('data_figures_nn/MyNet_MNIST.pt')       \n",
    "\n",
    "# model\n",
    "model = net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.8)   # momentum [0,1]\n",
    "\n",
    "\n",
    "# training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #model.eval()\n",
    "    with tqdm(train_data, unit=\"batch\") as tepoch:\n",
    "        # in our train data we have the pictures (data), target number 1-9 and bates of size 64 in this case\n",
    "        for (data, target) in tepoch:\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "            #data = Variable(data)\n",
    "            #target = Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            out = model(data)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = F.nll_loss(out, target)          # criterion = nn.CrossEntropyLoss() + loss = criterion(out, target), F.nll_loss is better\n",
    "            predictions = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == target).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # display training progress\n",
    "            #print_progress(epoch, batch_id, accuracy, loss)      # faster?\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            #time.sleep(0.1)\n",
    "            torch.save(net, 'data_figures_nn/MyNet_MNIST.pt')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data:\n",
    "        out = model(data)\n",
    "        loss += F.nll_loss(out, target, size_average=False).item()\n",
    "        prediction = out.data.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(target.data.view_as(prediction)).sum()\n",
    "    \n",
    "    loss = loss / len(test_data.dataset)\n",
    "    print('average loss: ', loss)\n",
    "    print('accuracy: ', int(100.*correct/len(test_data.dataset)), '%')\n",
    "        \n",
    "    \n",
    "for epoch in range(1,3):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e170",
   "metadata": {},
   "source": [
    "Some other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428883f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nameToTensor() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5940/1906945738.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTrainData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5940/1906945738.py\u001b[0m in \u001b[0;36mgetTrainData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlangs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mname_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnameToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mlang_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: nameToTensor() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import torch\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "letters = string.ascii_letters + \".,:''\"\n",
    "let = letters.find('a')\n",
    "#print(let)\n",
    "\n",
    "name = 'hallo'\n",
    "tens = torch.zeros(len(name),1,len(letters))\n",
    "#print(tens)\n",
    "\n",
    "# just that it is defined\n",
    "data = {}\n",
    "lang = 'German'\n",
    "langs = []\n",
    "langs.append(lang)\n",
    "data[lang] = name\n",
    "\n",
    "def nameToTensor():\n",
    "    pass\n",
    "\n",
    "# RNN\n",
    "class netRNN(nn.Module):\n",
    "    def __init__(self, inpu, hiddens, output):             # Grösse des Input, output, hiddens\n",
    "        super(netRNN, self).__init__()\n",
    "        # RNN selber schreiben\n",
    "        self.hiddens = hiddens\n",
    "        self.hid = nn.Linear(inpu + hiddens, hiddens)      # produziert neue hiddens\n",
    "        self.out = nn.Linear(inpu + hiddens, output)        # produziert neue outputs\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.cat((x, hidden), dim=1)                      # dimension 1, concatenation (Verkettung)\n",
    "        new_hidden = self.hid(x) \n",
    "        output = self.logsoftmax(self.out(x))\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1,self.hiddens))\n",
    "    \n",
    "    \n",
    "rnnModel = netRNN(len(letters), 128, len(data))                # net initialisieren\n",
    "\n",
    "\n",
    "def getTrainData():\n",
    "    lang = random.choice(langs)\n",
    "    name = random.choice(data[lang])\n",
    "    name_tensor = Variable(nameToTensor(name))\n",
    "    lang_tensor = Variable(torch.LongTensor([langs.index(lang)]))\n",
    "    return name, lang, name_tensor, lang_tensor\n",
    "\n",
    "# train network with chars that you can put different lengths of words in it!!!!\n",
    "# this is probably also favoriable for video with different amount of frames\n",
    "criterion = nn.NLLLoss()\n",
    "def train(lang_tensor, name_tensor):\n",
    "    hidden = rnnModel.initHidden()\n",
    "    rnnModel.zero_grad()\n",
    "    for i in range(name_tensor.size()[0]):\n",
    "        output, hidden = rnnModel(Name_tensor[i], hidden)\n",
    "    loss = criterion(output, lang_tensor)\n",
    "    loss.backward()\n",
    "    for i in rnnModel.parameters():         # optimizer manuel\n",
    "        i.data.add_(-0.01, i.grad.data)     # gradient decent, darauf addieren\n",
    "        \n",
    "    #print(loss)\n",
    "    return output\n",
    "\n",
    "sum = 0\n",
    "for i in range(1, 100000):\n",
    "    lang, name, lang_tensor, name_tensor = getTrainData()\n",
    "    output, loss = train(lang_tensor, name_tensor)\n",
    "    sum += loss.data[0]\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        avg.append(sum/1000)\n",
    "        sum = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(avg)\n",
    "plt.show()\n",
    "\n",
    "def sampleOut():\n",
    "    # input\n",
    "    hidden = model.initHidden()\n",
    "    output = 'a'\n",
    "    for i in range(15):\n",
    "        # nach training sample generieren\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c24700",
   "metadata": {},
   "source": [
    "Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62d13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/2122196588.py:123: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/2122196588.py:134: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  non_final_next = Variable(torch.cat([s for s in batch[2] if s is not None]), volatile=True)\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/2122196588.py:141: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  next_value[non_final] = model(non_final_next).max(1)[0]\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/2122196588.py:142: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  next_value.volatile = False\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/2122196588.py:144: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.smooth_l1_loss(action_value, target_action_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "epoch: 15\n",
      "epoch: 20\n",
      "epoch: 25\n",
      "epoch: 30\n",
      "epoch: 35\n",
      "epoch: 40\n",
      "epoch: 45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "from itertools import count\n",
    "\n",
    "# environment\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "# dass es überall läuft\n",
    "if 'inline' in matplotlib.get_backend():\n",
    "    from IPython import display\n",
    "    \n",
    "# interactive plotting\n",
    "plt.ion()\n",
    "\n",
    "# all tensors on the GPU\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        # cyclic list\n",
    "        self.pos = 0\n",
    "        \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.pos] = (state, action, next_state, reward)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,16, kernel_size=5, stride=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)    \n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.fc = nn.Linear(448,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "    \n",
    "resize = T.Compose([\n",
    "    T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()\n",
    "])\n",
    "    \n",
    "width = 600\n",
    "\n",
    "def cart_pos():\n",
    "    env_width = env.x_threshold * 2\n",
    "    return int(env.state[0] * width/env_width + width / 2.0)\n",
    "\n",
    "def get_image():\n",
    "    # wir brauchen CHW (channel width, height), wir bekommen ein anderes Format \n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2,0,1)\n",
    "    )\n",
    "    screen = screen[:, 160:320]\n",
    "    view = 320\n",
    "    cart = cart_pos()\n",
    "    # center the cart\n",
    "    if cart < view // 2:            # floor division --> abgerundet\n",
    "        sliced = slice(view)\n",
    "    elif cart > width - view // 2:\n",
    "        sliced = slice(-1*view, None)\n",
    "    else:\n",
    "        sliced = slice(cart - view // 2, cart + view // 2)\n",
    "    screen = screen[:,:,sliced]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "        \n",
    "#env.reset()\n",
    "#plt.figure()\n",
    "#plt.imshow(get_image().cpu().squeeze(0).permute(1,2,0).numpy(), interpolation='none')\n",
    "#plt.show()\n",
    "\n",
    "# model\n",
    "model = net()\n",
    "#model = model.cuda()\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "mem = Memory(16300)\n",
    "done = 0\n",
    "\n",
    "eps_end = 0.95\n",
    "eps_start = 0.95\n",
    "eps_steps = 150\n",
    "batch_size = 128\n",
    "# for reward\n",
    "gamma = 0.99\n",
    "def get_action(state):\n",
    "    global done\n",
    "    epsilon = random.random()\n",
    "    threshold = (eps_end + eps_start - eps_end) * math.exp(-1. * done / eps_steps)\n",
    "    done = done + 1\n",
    "    if epsilon > threshold:\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1,1)\n",
    "    else:\n",
    "        return LongTensor([[random.randint(0,1)]])\n",
    "\n",
    "# q - learning\n",
    "def train():\n",
    "    if len(mem) < batch_size:\n",
    "        return\n",
    "    x = mem.sample(batch_size)         # (state, action, next_state, reward)\n",
    "    batch = tuple(zip(*x))                 # ((s1,s2, ...), (a1,a2,...), (n1, n2, ...), (r1,r2,...))\n",
    "    non_final = ByteTensor(tuple(map(lambda s: s is not None, batch[2])))\n",
    "    non_final_next = Variable(torch.cat([s for s in batch[2] if s is not None]), volatile=True)\n",
    "    state = Variable(torch.cat(batch[0]))\n",
    "    action = Variable(torch.cat(batch[1]))\n",
    "    reward = Variable(torch.cat(batch[3]))\n",
    "    action_value = model(state).gather(1, action)\n",
    "    # v - value\n",
    "    next_value = Variable(torch.zeros(batch_size).type(FloatTensor))\n",
    "    next_value[non_final] = model(non_final_next).max(1)[0]\n",
    "    next_value.volatile = False\n",
    "    target_action_value = (next_value * gamma) + reward\n",
    "    loss = F.smooth_l1_loss(action_value, target_action_value)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "made_it = []\n",
    "train_duration = 50\n",
    "for i in range(train_duration):\n",
    "    if i % (train_duration / 10) == 0:\n",
    "        print('epoch:', i)\n",
    "    env.reset()\n",
    "    last = get_image()\n",
    "    current = get_image()\n",
    "    state = current - last\n",
    "    for j in count():\n",
    "        action = get_action(state)\n",
    "        _,reward, lost, _ = env.step(int(action[0,0]))\n",
    "        reward = Tensor([reward])\n",
    "        \n",
    "        last = current\n",
    "        current = get_image()\n",
    "        if lost:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = current - last\n",
    "        \n",
    "        mem.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        train()\n",
    "        if lost:\n",
    "            made_it.append(j)\n",
    "            #show()\n",
    "            break\n",
    "        \n",
    "        \n",
    "#env.render(close= True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.figure()\n",
    "plt.plot(made_it)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351a2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
