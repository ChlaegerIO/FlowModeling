{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8f0b7e",
   "metadata": {},
   "source": [
    "PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b0ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print progress, possibly faster than progress bar\n",
    "def print_progress(epoch, batch_id, accuracy, loss):\n",
    "    progress = '='* int((10. * batch_id / len(train_data)))\n",
    "    progress += '>'\n",
    "    if batch_id == 1:\n",
    "        print('Train Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='')\n",
    "    else:\n",
    "        print('\\rTrain Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f332b95",
   "metadata": {},
   "source": [
    "Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83753ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing odd number 9: 100%|███████████████████████████████████| 10/10 [00:05<00:00,  1.97carrots/s, divisors=[1, 3, 9]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "def get_divisors(n):\n",
    "    divisors = []\n",
    "    for m in range(1, n+1):\n",
    "        if n % m == 0:\n",
    "            divisors.append(m)\n",
    "    return divisors\n",
    "\n",
    "iterations = 10\n",
    "with trange(iterations, unit=\"carrots\") as pbar:\n",
    "    for i in pbar:\n",
    "        sleep(0.5)\n",
    "        if i % 2:\n",
    "            pbar.set_description(f\"Testing odd number {i}\")\n",
    "        else:\n",
    "            pbar.set_description(f\"Testing even number {i}\")\n",
    "        pbar.set_postfix(divisors=get_divisors(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8163b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9733, 1.6265, 2.4631],\n",
      "        [1.7833, 0.7271, 0.1025],\n",
      "        [0.8165, 1.9885, 2.2756],\n",
      "        [0.1511, 1.4171, 0.6514],\n",
      "        [1.5482, 2.5004, 0.0784]])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'catdog/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20048/1410374020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mtarget_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'catdog/train/'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# Tutorial: https://www.youtube.com/watch?v=GBzojftwfGQ&list=PLNmsVeXQZj7rx55Mai21reZtd_8m-qe27&index=5&ab_channel=TheMorpheusTutorials\n",
    "# 1. Tensor\n",
    "x = torch.Tensor(5,3)   # numpy arrays werden durch Tensoren ersetzt\n",
    "x[:,:] = 1\n",
    "y = torch.randn(5,3)    # random tenseor\n",
    "#print(y)\n",
    "xy = torch.add(x,y)\n",
    "print(xy)\n",
    "\n",
    "# 2. Für Grafik Karte\n",
    "#if torch.cuda.is_available():\n",
    "#    x = x.cuda()       # RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable \n",
    "#    y = y.cude()\n",
    "#print(x+y)\n",
    "\n",
    "# 3. Daten laden \n",
    "kwargs = {}\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)\n",
    "# Bilder laden und verarbeiten\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from os import listdir\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])    # normalize around mean with sigma (std)\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), normalize])\n",
    "train_data_list = []\n",
    "train_data = []\n",
    "target_label = []\n",
    "for f in listdir('catdog/train/'):\n",
    "    img = Image.open('catdog/train/' + f)\n",
    "    img_tensor = transforms(img)\n",
    "    img_tensor.unsqueeze_(0)   # save directly in img_tensor with _ at the end (1,3,256,256)\n",
    "    # label\n",
    "    isCat = 1 if 'cat' in f else 0\n",
    "    isDog = 1 if 'dog' in f else 0\n",
    "    target = [isCat, isDog]\n",
    "    target_label.append(target)\n",
    "    if len(train_data_list) >= 64:\n",
    "        train_data.append(torch.stack(train_data_list), target_label)\n",
    "        train_data_list = []\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591cf2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 4. Erste eigene Neuronale Netzwerk\n",
    "class MyNet(nn.Module): # Vererbungslehre\n",
    "    def __init__(self):              # initialize network\n",
    "        super(MyNet,self).__init__()\n",
    "        self.lin1 = nn.Linear(10,10)\n",
    "        self.lin2 = nn.Linear(10,10)\n",
    "    \n",
    "    def forward(self, x):            # forward pass function\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size() [1:]         # ohne batch dimension\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num\n",
    "    \n",
    "net = MyNet()\n",
    "#net = net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e629a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4419, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1214, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1770e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7166e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4889e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4567e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5887e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8590e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2454e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7293e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2954e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9305e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6237e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3656e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1485e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.6599e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1247e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8336e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7478e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8344e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0662e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4201e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8768e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4197e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0352e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7119e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4399e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2112e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0188e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5691e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2079e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0628e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0997e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2896e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6081e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0348e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5528e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1474e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8063e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5193e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2780e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0750e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0434e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6051e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3985e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3819e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5267e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.0230, -0.0031, -0.0313, -0.0392,  1.1351, -0.0732, -0.0094,  0.0220,\n",
      "         1.0155,  1.0232], grad_fn=<AddBackward0>)\n",
      "tensor([[ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 5. main training loop with input\n",
    "for i in range(100):\n",
    "    x = [0,1,1,1,0,1,1,1,0,0]\n",
    "    input = Variable(torch.Tensor([x for _ in range(10)]))    # 10x10 input variable, weil man ein batch von 10 daraus erstellt\n",
    "    out = net(input)                      # input durch Netz durchlaufen lassen\n",
    "\n",
    "    # labels what to expect\n",
    "    x = [1,0,0,0,1,0,0,0,1,1]\n",
    "    target = Variable(torch.Tensor([x for _ in range(10)]))\n",
    "    # loss\n",
    "    criterion = nn.MSELoss()              # Fehlerfunktionen: optimieren (softwmax, MSE, ...)\n",
    "    loss = criterion(out, target)\n",
    "    print(loss)\n",
    "\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1,)            # optimizer ausprobieren SGD, Adam, ...\n",
    "    optimizer.step()\n",
    "    y\n",
    "    \n",
    "test_input = Variable(torch.Tensor([1,1,1,1,0,0,0,1,1,1]))\n",
    "test_out = net(test_input)\n",
    "print(test_out)  \n",
    "\n",
    "print(out)\n",
    "\n",
    "# 6. Speichern und Laden des Trainierten Netzes (pro Zyklus speichern, falls das Programm abstürtzt während dem trainieren)\n",
    "torch.save(net, 'data_figures_nn/MyNet.pt')\n",
    "if os.path.isfile('data_figures_nn/MyNet.pt'):\n",
    "    net_load = torch.load('data_figures_nn/MyNet.pt')\n",
    "\n",
    "print(net_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b82887",
   "metadata": {},
   "source": [
    "MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f6c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data_figures_nn\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3082,))\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████| 938/938 [00:55<00:00, 16.96batch/s, accuracy=45.3, loss=0.203]\n",
      "c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.0785710596010089\n",
      "accuracy:  97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████| 938/938 [00:55<00:00, 17.03batch/s, accuracy=45.3, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.08161891804933548\n",
      "accuracy:  97 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "kwargs = {}          # without GPU\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True}        # with GPU\n",
    "batch_size=64\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "print(len(train_data.dataset))\n",
    "\n",
    "test_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=False, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "print(train_data.dataset)\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        # definition of object layers\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)    # (in_channels: Eingangsdimension, out_channels: Ausgangsdimension)\n",
    "        # (kernel_size: Faltungsfiltergrösse, stride: #Schritte der Faltung (reduziert Dimension), padding: Leerer Rand dass Dimension weniger abnimmt, dilation: adding zeros between kernel, groups: , bias: added to output, padding_mode='zeros')\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        # (p=0.5: probability for dropout, inplace: if True input layer gets changed)\n",
    "        self.conv_dropout = nn.Dropout2d()            # damit das Netz nichts auswendig lernt\n",
    "        self.fc1 = nn.Linear(320, 60)\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # actual layer composition\n",
    "        #print('start:',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        # (kernel_size, stride, padding, dilation, return_indices: of max value if True, ceil_mode: ceil if True (aufrunden))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool1:', x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1:',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:',x.shape)\n",
    "        x = self.conv_dropout(x)\n",
    "        #print('dropout:',x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool2:',x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2:', x.shape)\n",
    "        x = x.view(-1,320)        # daten umformen\n",
    "        #print('view:', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print('relu3_fc1:', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print('fc2:', x.shape)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "if os.path.isfile('data_figures_nn/MyNet_MNIST.pt'):\n",
    "    net = torch.load('data_figures_nn/MyNet_MNIST.pt')       \n",
    "\n",
    "# model\n",
    "model = net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.8)   # momentum [0,1]\n",
    "\n",
    "\n",
    "# training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #model.eval()\n",
    "    with tqdm(train_data, unit=\"batch\") as tepoch:\n",
    "        # in our train data we have the pictures (data), target number 1-9 and bates of size 64 in this case\n",
    "        for (data, target) in tepoch:\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "            #data = Variable(data)\n",
    "            #target = Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            out = model(data)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = F.nll_loss(out, target)          # criterion = nn.CrossEntropyLoss() + loss = criterion(out, target), F.nll_loss is better\n",
    "            predictions = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == target).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # display training progress\n",
    "            #print_progress(epoch, batch_id, accuracy, loss)      # faster?\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            #time.sleep(0.1)\n",
    "            torch.save(net, 'data_figures_nn/MyNet_MNIST.pt')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data:\n",
    "        out = model(data)\n",
    "        loss += F.nll_loss(out, target, size_average=False).item()\n",
    "        prediction = out.data.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(target.data.view_as(prediction)).sum()\n",
    "    \n",
    "    loss = loss / len(test_data.dataset)\n",
    "    print('average loss: ', loss)\n",
    "    print('accuracy: ', int(100.*correct/len(test_data.dataset)), '%')\n",
    "        \n",
    "    \n",
    "for epoch in range(1,3):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e170",
   "metadata": {},
   "source": [
    "Some other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "428883f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20048/3908830574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mletters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# net initialisieren\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import torch\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "letters = string.ascii_letters + \".,:''\"\n",
    "let = letters.find('a')\n",
    "print(let)\n",
    "\n",
    "name = 'hallo'\n",
    "tens = torch.zeros(len(name),1,len(letters))\n",
    "print(tens)\n",
    "\n",
    "\n",
    "\n",
    "# RNN\n",
    "class netRNN(nn.Module):\n",
    "    def __init__(self, inpu, hiddens, output):             # Grösse des Input, output, hiddens\n",
    "        super(netRNN, self).__init__()\n",
    "        # RNN selber schreiben\n",
    "        self.hiddens = hiddens\n",
    "        self.hid = nn.Linear(inpu + hiddens, hiddens)      # produziert neue hiddens\n",
    "        self.out = nn.Linear(inpu + hiddens, ouput)        # produziert neue outputs\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.cat((x, hidden), 1)                      # dimension 1\n",
    "        new_hidden = self.hid(x) \n",
    "        output = self.logsoftmax(self.out(x))\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1,self.hiddens))\n",
    "    \n",
    "    \n",
    "rnnModel = netRNN(len(letters), 128, len(data))                # net initialisieren\n",
    "\n",
    "\n",
    "def getTrainData():\n",
    "    lang = random.choice(langs)\n",
    "    name = random.choice(data[lang])\n",
    "    return name, lang\n",
    "\n",
    "# train network with chars that you can put different lengths of words in it!!!!\n",
    "# this is probably also favoriable for video with different amount of frames\n",
    "criterion = nn.NLLLoss()\n",
    "def train(lang_tensor, name_tensor):\n",
    "    hidden = rnnModel.initHidden()\n",
    "    rnnModel.zero_grad()\n",
    "    for i in range(name_tensor.size()[0]):\n",
    "        output, hidden = rnnModel(Name_tensor[i], hidden)\n",
    "    loss = criterion(output, lang_tensor)\n",
    "    loss.backward()\n",
    "    for i in rnnModel.parameters():         # optimizer manuel\n",
    "        i.data.add_(-0.01, i.grad.data)     # gradient decent, darauf addieren\n",
    "        \n",
    "    print(loss)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f0d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
