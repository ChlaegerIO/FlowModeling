{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d82b718",
   "metadata": {},
   "source": [
    "PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47968ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print progress, possibly faster than progress bar\n",
    "def print_progress(epoch, batch_id, accuracy, loss):\n",
    "    progress = '='* int((10. * batch_id / len(train_data)))\n",
    "    progress += '>'\n",
    "    if batch_id == 1:\n",
    "        print('Train Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='')\n",
    "    else:\n",
    "        print('\\rTrain Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d79fcb",
   "metadata": {},
   "source": [
    "Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4167512a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing odd number 9: 100%|███████████████████████████████████| 10/10 [00:05<00:00,  1.99carrots/s, divisors=[1, 3, 9]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "def get_divisors(n):\n",
    "    divisors = []\n",
    "    for m in range(1, n+1):\n",
    "        if n % m == 0:\n",
    "            divisors.append(m)\n",
    "    return divisors\n",
    "\n",
    "iterations = 10\n",
    "with trange(iterations, unit=\"carrots\") as pbar:\n",
    "    for i in pbar:\n",
    "        sleep(0.5)\n",
    "        if i % 2:\n",
    "            pbar.set_description(f\"Testing odd number {i}\")\n",
    "        else:\n",
    "            pbar.set_description(f\"Testing even number {i}\")\n",
    "        pbar.set_postfix(divisors=get_divisors(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f1e8c",
   "metadata": {},
   "source": [
    "Example MINST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15956a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3834,  0.2679, -0.5379],\n",
      "        [ 1.5807,  0.9722,  1.5772],\n",
      "        [-0.4374,  3.2227,  0.9192],\n",
      "        [ 2.2728,  1.2195, -0.5141],\n",
      "        [ 0.5376,  0.2940, -0.0825]])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'catdog/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15000/3536797541.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mtarget_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'catdog/train/'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# Tutorial: https://www.youtube.com/watch?v=GBzojftwfGQ&list=PLNmsVeXQZj7rx55Mai21reZtd_8m-qe27&index=5&ab_channel=TheMorpheusTutorials\n",
    "# 1. Tensor\n",
    "x = torch.Tensor(5,3)   # numpy arrays werden durch Tensoren ersetzt\n",
    "x[:,:] = 1\n",
    "y = torch.randn(5,3)    # random tenseor\n",
    "#print(y)\n",
    "xy = torch.add(x,y)\n",
    "print(xy)\n",
    "\n",
    "# 2. Für Grafik Karte\n",
    "#if torch.cuda.is_available():\n",
    "#    x = x.cuda()       # RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable \n",
    "#    y = y.cude()\n",
    "#print(x+y)\n",
    "\n",
    "# 3. Daten laden \n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)\n",
    "# Bilder laden und verarbeiten\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from os import listdir\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])    # normalize around mean with sigma (std)\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), normalize])\n",
    "train_data_list = []\n",
    "train_data = []\n",
    "target_label = []\n",
    "for f in listdir('catdog/train/'):\n",
    "    img = Image.open('catdog/train/' + f)\n",
    "    img_tensor = transforms(img)\n",
    "    img_tensor.unsqueeze_(0)   # save directly in img_tensor with _ at the end (1,3,256,256)\n",
    "    # label\n",
    "    isCat = 1 if 'cat' in f else 0\n",
    "    isDog = 1 if 'dog' in f else 0\n",
    "    target = [isCat, isDog]\n",
    "    target_label.append(target)\n",
    "    if len(train_data_list) >= 64:\n",
    "        train_data.append(torch.stack(train_data_list), target_label)\n",
    "        train_data_list = []\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b474d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 4. Erste eigene Neuronale Netzwerk\n",
    "class MyNet(nn.Module): # Vererbungslehre\n",
    "    def __init__(self):              # initialize network\n",
    "        super(MyNet,self).__init__()\n",
    "        self.lin1 = nn.Linear(10,10)\n",
    "        self.lin2 = nn.Linear(10,10)\n",
    "    \n",
    "    def forward(self, x):            # forward pass function\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size() [1:]         # ohne batch dimension\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num\n",
    "    \n",
    "net = MyNet()\n",
    "#net = net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dcd8423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3283, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2557, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2456, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2265, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2176, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2089, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1927, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1851, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1778, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1640, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1575, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1512, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1452, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1395, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1340, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1287, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0353, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.0704, -0.2201, -0.0348,  0.0104,  0.5934, -0.1743,  0.5694, -0.0192,\n",
      "         0.7797,  0.5452], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843],\n",
      "        [ 0.9061,  0.0343,  0.0088, -0.0106,  0.8648, -0.0371,  0.0210,  0.0204,\n",
      "          0.8952,  0.8843]], grad_fn=<AddmmBackward0>)\n",
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 5. main training loop with input\n",
    "for i in range(100):\n",
    "    x = [0,1,1,1,0,1,1,1,0,0]\n",
    "    input = Variable(torch.Tensor([x for _ in range(10)]))    # 10x10 input variable, weil man ein batch von 10 daraus erstellt\n",
    "    out = net(input)                      # input durch Netz durchlaufen lassen\n",
    "\n",
    "    # labels what to expect\n",
    "    x = [1,0,0,0,1,0,0,0,1,1]\n",
    "    target = Variable(torch.Tensor([x for _ in range(10)]))\n",
    "    # loss\n",
    "    criterion = nn.MSELoss()              # Fehlerfunktionen: optimieren (softwmax, MSE, ...)\n",
    "    loss = criterion(out, target)\n",
    "    print(loss)\n",
    "\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1,)            # optimizer ausprobieren SGD, Adam, ...\n",
    "    optimizer.step()\n",
    "    y\n",
    "    \n",
    "test_input = Variable(torch.Tensor([1,1,1,1,0,0,0,1,1,1]))\n",
    "test_out = net(test_input)\n",
    "print(test_out)  \n",
    "\n",
    "print(out)\n",
    "\n",
    "# 6. Speichern und Laden des Trainierten Netzes (pro Zyklus speichern, falls das Programm abstürtzt während dem trainieren)\n",
    "torch.save(net, 'data_figures_nn/MyNet.pt')\n",
    "if os.path.isfile('data_figures_nn/MyNet.pt'):\n",
    "    net_load = torch.load('data_figures_nn/MyNet.pt')\n",
    "\n",
    "print(net_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470b4dd",
   "metadata": {},
   "source": [
    "MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7b41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████| 938/938 [00:30<00:00, 30.73batch/s, accuracy=46.9, loss=0.0942]\n",
      "c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.10801963654011489\n",
      "accuracy:  97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████| 938/938 [00:31<00:00, 29.67batch/s, accuracy=48.4, loss=0.0802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.10604452992081642\n",
      "accuracy:  97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████| 938/938 [00:53<00:00, 17.43batch/s, accuracy=46.9, loss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.08309069368974306\n",
      "accuracy:  97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████| 938/938 [01:03<00:00, 14.73batch/s, accuracy=45.3, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.06763213026747107\n",
      "accuracy:  98 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "kwargs = {}          # without GPU\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True}        # with GPU\n",
    "batch_size=64\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "\n",
    "test_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=False, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        # definition of object layers\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)    # (in_channels: Eingangsdimension, out_channels: Ausgangsdimension)\n",
    "        # (kernel_size: Faltungsfiltergrösse, stride: #Schritte der Faltung (reduziert Dimension), padding: Leerer Rand dass Dimension weniger abnimmt, dilation: adding zeros between kernel, groups: , bias: added to output, padding_mode='zeros')\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        # (p=0.5: probability for dropout, inplace: if True input layer gets changed)\n",
    "        self.conv_dropout = nn.Dropout2d()            # damit das Netz nichts auswendig lernt\n",
    "        self.fc1 = nn.Linear(320, 60)\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # actual layer composition\n",
    "        #print('start:',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        # (kernel_size, stride, padding, dilation, return_indices: of max value if True, ceil_mode: ceil if True (aufrunden))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool1:', x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1:',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:',x.shape)\n",
    "        x = self.conv_dropout(x)\n",
    "        #print('dropout:',x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool2:',x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2:', x.shape)\n",
    "        x = x.view(-1,320)        # daten umformen\n",
    "        #print('view:', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print('relu3_fc1:', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print('fc2:', x.shape)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "if os.path.isfile('data_figures_nn/MyNet_MNIST.pt'):\n",
    "    net = torch.load('data_figures_nn/MyNet_MNIST.pt')       \n",
    "\n",
    "# model\n",
    "model = net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.8)   # momentum [0,1]\n",
    "\n",
    "\n",
    "# training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #model.eval()\n",
    "    with tqdm(train_data, unit=\"batch\") as tepoch:\n",
    "        # in our train data we have the pictures (data), target number 1-9 and bates of size 64 in this case\n",
    "        for (data, target) in tepoch:\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "            #data = Variable(data)\n",
    "            #target = Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            out = model(data)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = F.nll_loss(out, target)          # criterion = nn.CrossEntropyLoss() + loss = criterion(out, target), F.nll_loss is better\n",
    "            predictions = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == target).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # display training progress\n",
    "            #print_progress(epoch, batch_id, accuracy, loss)      # faster?\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            #time.sleep(0.1)\n",
    "            torch.save(net, 'data_figures_nn/MyNet_MNIST.pt')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data:\n",
    "        out = model(data)\n",
    "        loss += F.nll_loss(out, target, size_average=False).item()\n",
    "        prediction = out.data.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(target.data.view_as(prediction)).sum()\n",
    "    \n",
    "    loss = loss / len(test_data.dataset)\n",
    "    print('average loss: ', loss)\n",
    "    print('accuracy: ', int(100.*correct/len(test_data.dataset)), '%')\n",
    "        \n",
    "    \n",
    "for epoch in range(1,5):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8341cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
