{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8f0b7e",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b0ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print progress, possibly faster than progress bar\n",
    "def print_progress(epoch, batch_id, accuracy, loss):\n",
    "    progress = '='* int((10. * batch_id / len(train_data)))\n",
    "    progress += '>'\n",
    "    if batch_id == 1:\n",
    "        print('Train Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='')\n",
    "    else:\n",
    "        print('\\rTrain Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch, 100. * batch_id / len(train_data),progress, accuracy, loss.item()), end='', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f332b95",
   "metadata": {},
   "source": [
    "### Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83753ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing odd number 9: 100%|███████████████████████████████████| 10/10 [00:05<00:00,  1.97carrots/s, divisors=[1, 3, 9]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "def get_divisors(n):\n",
    "    divisors = []\n",
    "    for m in range(1, n+1):\n",
    "        if n % m == 0:\n",
    "            divisors.append(m)\n",
    "    return divisors\n",
    "\n",
    "iterations = 10\n",
    "with trange(iterations, unit=\"carrots\") as pbar:\n",
    "    for i in pbar:\n",
    "        sleep(0.5)\n",
    "        if i % 2:\n",
    "            pbar.set_description(f\"Testing odd number {i}\")\n",
    "        else:\n",
    "            pbar.set_description(f\"Testing even number {i}\")\n",
    "        pbar.set_postfix(divisors=get_divisors(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf27e75",
   "metadata": {},
   "source": [
    "### First network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8163b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9733, 1.6265, 2.4631],\n",
      "        [1.7833, 0.7271, 0.1025],\n",
      "        [0.8165, 1.9885, 2.2756],\n",
      "        [0.1511, 1.4171, 0.6514],\n",
      "        [1.5482, 2.5004, 0.0784]])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'catdog/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20048/1410374020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mtarget_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catdog/train/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'catdog/train/'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import os\n",
    "\n",
    "# Tutorial: https://www.youtube.com/watch?v=GBzojftwfGQ&list=PLNmsVeXQZj7rx55Mai21reZtd_8m-qe27&index=5&ab_channel=TheMorpheusTutorials\n",
    "# 1. Tensor\n",
    "x = torch.Tensor(5,3)   # numpy arrays werden durch Tensoren ersetzt\n",
    "x[:,:] = 1\n",
    "y = torch.randn(5,3)    # random tenseor\n",
    "#print(y)\n",
    "xy = torch.add(x,y)\n",
    "print(xy)\n",
    "\n",
    "# 2. Für Grafik Karte\n",
    "#if torch.cuda.is_available():\n",
    "#    x = x.cuda()       # RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable \n",
    "#    y = y.cude()\n",
    "#print(x+y)\n",
    "\n",
    "# 3. Daten laden \n",
    "kwargs = {}\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)\n",
    "# Bilder laden und verarbeiten\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from os import listdir\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])    # normalize around mean with sigma (std)\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), normalize])\n",
    "train_data_list = []\n",
    "train_data = []\n",
    "target_label = []\n",
    "for f in listdir('catdog/train/'):        # choose your datasource\n",
    "    img = Image.open('catdog/train/' + f)\n",
    "    img_tensor = transforms(img)\n",
    "    img_tensor.unsqueeze_(0)   # save directly in img_tensor with _ at the end (1,3,256,256)\n",
    "    # label\n",
    "    isCat = 1 if 'cat' in f else 0\n",
    "    isDog = 1 if 'dog' in f else 0\n",
    "    target = [isCat, isDog]\n",
    "    target_label.append(target)\n",
    "    if len(train_data_list) >= 64:\n",
    "        train_data.append(torch.stack(train_data_list), target_label)\n",
    "        train_data_list = []\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591cf2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 4. Erste eigene Neuronale Netzwerk\n",
    "class MyNet(nn.Module): # Vererbungslehre\n",
    "    def __init__(self):              # initialize network\n",
    "        super(MyNet,self).__init__()\n",
    "        self.lin1 = nn.Linear(10,10)\n",
    "        self.lin2 = nn.Linear(10,10)\n",
    "    \n",
    "    def forward(self, x):            # forward pass function\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size() [1:]         # ohne batch dimension\n",
    "        num = 1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num\n",
    "    \n",
    "net = MyNet()\n",
    "#net = net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e629a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4419, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1214, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1770e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7166e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4889e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4567e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5887e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8590e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2454e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7293e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2954e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9305e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6237e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3656e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1485e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.6599e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1247e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.8336e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7478e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8344e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0662e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4201e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8768e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4197e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0352e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7119e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4399e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2112e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0188e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5691e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2079e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0628e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0997e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2896e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6081e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0348e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5528e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1474e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8063e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5193e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2780e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0750e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0434e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6051e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.3985e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3819e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5267e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.0230, -0.0031, -0.0313, -0.0392,  1.1351, -0.0732, -0.0094,  0.0220,\n",
      "         1.0155,  1.0232], grad_fn=<AddBackward0>)\n",
      "tensor([[ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00],\n",
      "        [ 1.0002e+00,  8.5995e-05,  6.3628e-06,  2.8774e-05,  9.9954e-01,\n",
      "         -1.3781e-04, -1.5809e-04, -3.3108e-04,  9.9994e-01,  1.0002e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "MyNet(\n",
      "  (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 5. main training loop with input\n",
    "for i in range(100):\n",
    "    x = [0,1,1,1,0,1,1,1,0,0]\n",
    "    input = Variable(torch.Tensor([x for _ in range(10)]))    # 10x10 input variable, weil man ein batch von 10 daraus erstellt\n",
    "    out = net(input)                      # input durch Netz durchlaufen lassen\n",
    "\n",
    "    # labels what to expect\n",
    "    x = [1,0,0,0,1,0,0,0,1,1]\n",
    "    target = Variable(torch.Tensor([x for _ in range(10)]))\n",
    "    # loss\n",
    "    criterion = nn.MSELoss()              # Fehlerfunktionen: optimieren (softwmax, MSE, ...)\n",
    "    loss = criterion(out, target)\n",
    "    print(loss)\n",
    "\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1,)            # optimizer ausprobieren SGD, Adam, ...\n",
    "    optimizer.step()\n",
    "    y\n",
    "    \n",
    "test_input = Variable(torch.Tensor([1,1,1,1,0,0,0,1,1,1]))\n",
    "test_out = net(test_input)\n",
    "print(test_out)  \n",
    "\n",
    "print(out)\n",
    "\n",
    "# 6. Speichern und Laden des Trainierten Netzes (pro Zyklus speichern, falls das Programm abstürtzt während dem trainieren)\n",
    "torch.save(net, 'data_figures_nn/MyNet.pt')\n",
    "if os.path.isfile('data_figures_nn/MyNet.pt'):\n",
    "    net_load = torch.load('data_figures_nn/MyNet.pt')\n",
    "\n",
    "print(net_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b82887",
   "metadata": {},
   "source": [
    "### MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f6c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data_figures_nn\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3082,))\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████| 938/938 [00:55<00:00, 16.96batch/s, accuracy=45.3, loss=0.203]\n",
      "c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.0785710596010089\n",
      "accuracy:  97 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████| 938/938 [00:55<00:00, 17.03batch/s, accuracy=45.3, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss:  0.08161891804933548\n",
      "accuracy:  97 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "kwargs = {}          # without GPU\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True}        # with GPU\n",
    "batch_size=64\n",
    "train_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=True, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "print(len(train_data.dataset))\n",
    "\n",
    "test_data = torch.utils.data.DataLoader(datasets.MNIST('data_figures_nn', train=False, download=True, \n",
    "                                                    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3082,))])),\n",
    "                                    batch_size=64, shuffle=True, **kwargs)    # shuffle that network don't learns repetition, kwargs are arguments for the function (has to be written)  \n",
    "print(train_data.dataset)\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        # definition of object layers\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)    # (in_channels: Eingangsdimension, out_channels: Ausgangsdimension)\n",
    "        # (kernel_size: Faltungsfiltergrösse, stride: #Schritte der Faltung (reduziert Dimension), padding: Leerer Rand dass Dimension weniger abnimmt, dilation: adding zeros between kernel, groups: , bias: added to output, padding_mode='zeros')\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        # (p=0.5: probability for dropout, inplace: if True input layer gets changed)\n",
    "        self.conv_dropout = nn.Dropout2d()            # damit das Netz nichts auswendig lernt\n",
    "        self.fc1 = nn.Linear(320, 60)\n",
    "        self.fc2 = nn.Linear(60, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # actual layer composition\n",
    "        #print('start:',x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        # (kernel_size, stride, padding, dilation, return_indices: of max value if True, ceil_mode: ceil if True (aufrunden))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool1:', x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu1:',x.shape)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:',x.shape)\n",
    "        x = self.conv_dropout(x)\n",
    "        #print('dropout:',x.shape)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        #print('pool2:',x.shape)\n",
    "        x = F.relu(x)\n",
    "        #print('relu2:', x.shape)\n",
    "        x = x.view(-1,320)        # daten umformen\n",
    "        #print('view:', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print('relu3_fc1:', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print('fc2:', x.shape)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "if os.path.isfile('data_figures_nn/MyNet_MNIST.pt'):\n",
    "    net = torch.load('data_figures_nn/MyNet_MNIST.pt')       \n",
    "\n",
    "# model\n",
    "model = net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.8)   # momentum [0,1]\n",
    "\n",
    "\n",
    "# training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #model.eval()\n",
    "    with tqdm(train_data, unit=\"batch\") as tepoch:\n",
    "        # in our train data we have the pictures (data), target number 1-9 and bates of size 64 in this case\n",
    "        for (data, target) in tepoch:\n",
    "            #data = data.cuda()\n",
    "            #target = target.cuda()\n",
    "            #data = Variable(data)\n",
    "            #target = Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            out = model(data)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = F.nll_loss(out, target)          # criterion = nn.CrossEntropyLoss() + loss = criterion(out, target), F.nll_loss is better\n",
    "            predictions = out.argmax(dim=1, keepdim=True).squeeze()\n",
    "            correct = (predictions == target).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # display training progress\n",
    "            #print_progress(epoch, batch_id, accuracy, loss)      # faster?\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            #time.sleep(0.1)\n",
    "            torch.save(net, 'data_figures_nn/MyNet_MNIST.pt')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data:\n",
    "        out = model(data)\n",
    "        loss += F.nll_loss(out, target, size_average=False).item()\n",
    "        prediction = out.data.max(1, keepdim=True)[1]\n",
    "        correct += prediction.eq(target.data.view_as(prediction)).sum()\n",
    "    \n",
    "    loss = loss / len(test_data.dataset)\n",
    "    print('average loss: ', loss)\n",
    "    print('accuracy: ', int(100.*correct/len(test_data.dataset)), '%')\n",
    "        \n",
    "    \n",
    "for epoch in range(1,3):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e170",
   "metadata": {},
   "source": [
    "### RNN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428883f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nameToTensor() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5940/1906945738.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTrainData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5940/1906945738.py\u001b[0m in \u001b[0;36mgetTrainData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlangs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mname_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnameToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mlang_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlangs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: nameToTensor() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import torch\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "letters = string.ascii_letters + \".,:''\"\n",
    "let = letters.find('a')\n",
    "#print(let)\n",
    "\n",
    "name = 'hallo'\n",
    "tens = torch.zeros(len(name),1,len(letters))\n",
    "#print(tens)\n",
    "\n",
    "# just that it is defined\n",
    "data = {}\n",
    "lang = 'German'\n",
    "langs = []\n",
    "langs.append(lang)\n",
    "data[lang] = name\n",
    "\n",
    "def nameToTensor():\n",
    "    pass\n",
    "\n",
    "# RNN\n",
    "class netRNN(nn.Module):\n",
    "    def __init__(self, inpu, hiddens, output):             # Grösse des Input, output, hiddens\n",
    "        super(netRNN, self).__init__()\n",
    "        # RNN selber schreiben\n",
    "        self.hiddens = hiddens\n",
    "        self.hid = nn.Linear(inpu + hiddens, hiddens)      # produziert neue hiddens\n",
    "        self.out = nn.Linear(inpu + hiddens, output)        # produziert neue outputs\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = torch.cat((x, hidden), dim=1)                      # dimension 1, concatenation (Verkettung)\n",
    "        new_hidden = self.hid(x) \n",
    "        output = self.logsoftmax(self.out(x))\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1,self.hiddens))\n",
    "    \n",
    "    \n",
    "rnnModel = netRNN(len(letters), 128, len(data))                # net initialisieren\n",
    "\n",
    "\n",
    "def getTrainData():\n",
    "    lang = random.choice(langs)\n",
    "    name = random.choice(data[lang])\n",
    "    name_tensor = Variable(nameToTensor(name))\n",
    "    lang_tensor = Variable(torch.LongTensor([langs.index(lang)]))\n",
    "    return name, lang, name_tensor, lang_tensor\n",
    "\n",
    "# train network with chars that you can put different lengths of words in it!!!!\n",
    "# this is probably also favoriable for video with different amount of frames\n",
    "criterion = nn.NLLLoss()\n",
    "def train(lang_tensor, name_tensor):\n",
    "    hidden = rnnModel.initHidden()\n",
    "    rnnModel.zero_grad()\n",
    "    for i in range(name_tensor.size()[0]):\n",
    "        output, hidden = rnnModel(Name_tensor[i], hidden)\n",
    "    loss = criterion(output, lang_tensor)\n",
    "    loss.backward()\n",
    "    for i in rnnModel.parameters():         # optimizer manuel\n",
    "        i.data.add_(-0.01, i.grad.data)     # gradient decent, darauf addieren\n",
    "        \n",
    "    #print(loss)\n",
    "    return output\n",
    "\n",
    "sum = 0\n",
    "for i in range(1, 100000):\n",
    "    lang, name, lang_tensor, name_tensor = getTrainData()\n",
    "    output, loss = train(lang_tensor, name_tensor)\n",
    "    sum += loss.data[0]\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        avg.append(sum/1000)\n",
    "        sum = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(avg)\n",
    "plt.show()\n",
    "\n",
    "def sampleOut():\n",
    "    # input\n",
    "    hidden = model.initHidden()\n",
    "    output = 'a'\n",
    "    for i in range(15):\n",
    "        # nach training sample generieren\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eedd4",
   "metadata": {},
   "source": [
    "### Autoencoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "dataiter = iter(data_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(torch.min(images), torch.max(images))\n",
    "\n",
    "# repeatedly reduce the size\n",
    "class Autoencoder_Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128), # (N, 784) -> (N, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3) # -> N, 3\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "# Input [-1, +1] -> use nn.Tanh\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        # N, 1, 28, 28\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1), # -> N, 16, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # -> N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7) # -> N, 64, 1, 1\n",
    "        )\n",
    "        \n",
    "        # N , 64, 1, 1\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7), # -> N, 32, 7, 7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), # N, 16, 14, 14 (N,16,13,13 without output_padding)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), # N, 1, 28, 28  (N,1,27,27)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "\n",
    "# Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
    "# Input [-1, +1] -> use nn.Tanh\n",
    "\n",
    "model = Autoencoder()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Point to training loop video\n",
    "num_epochs = 1\n",
    "outputs = []\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "        # img = img.reshape(-1, 28*28) # -> use for Autoencoder_Linear\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n",
    "    outputs.append((epoch, img, recon))\n",
    "\n",
    "# plotting\n",
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].detach().numpy()\n",
    "    recon = outputs[k][2].detach().numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, i+1)\n",
    "        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])\n",
    "            \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 9: break\n",
    "        plt.subplot(2, 9, 9+i+1) # row_length + i + 1\n",
    "        # item = item.reshape(-1, 28,28) # -> use for Autoencoder_Linear\n",
    "        # item: 1, 28, 28\n",
    "        plt.imshow(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c24700",
   "metadata": {},
   "source": [
    "### Reinforcement learning example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62d13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x0000028803836670>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 328, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"c:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\chapter_book\\databook_python\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 857, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\timok\\Python\\py390\\lib\\_weakrefset.py\", line 110, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x000002887FDF5090; to 'Win32Window' at 0x000002887F9DBF40>\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/632359181.py:123: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/632359181.py:134: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  non_final_next = Variable(torch.cat([s for s in batch[2] if s is not None]), volatile=True)\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/632359181.py:141: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  next_value[non_final] = model(non_final_next).max(1)[0]\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/632359181.py:142: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  next_value.volatile = False\n",
      "C:\\Users\\timok\\AppData\\Local\\Temp/ipykernel_19244/632359181.py:144: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.smooth_l1_loss(action_value, target_action_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "epoch: 15\n",
      "epoch: 20\n",
      "epoch: 25\n",
      "epoch: 30\n",
      "epoch: 35\n",
      "epoch: 40\n",
      "epoch: 45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "from itertools import count\n",
    "\n",
    "# environment\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "# dass es überall läuft\n",
    "if 'inline' in matplotlib.get_backend():\n",
    "    from IPython import display\n",
    "    \n",
    "# interactive plotting\n",
    "plt.ion()\n",
    "\n",
    "# all tensors on the GPU\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        # cyclic list\n",
    "        self.pos = 0\n",
    "        \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.pos] = (state, action, next_state, reward)\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,16, kernel_size=5, stride=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)    \n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "        self.fc = nn.Linear(448,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "    \n",
    "resize = T.Compose([\n",
    "    T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()\n",
    "])\n",
    "    \n",
    "width = 600\n",
    "\n",
    "def cart_pos():\n",
    "    env_width = env.x_threshold * 2\n",
    "    return int(env.state[0] * width/env_width + width / 2.0)\n",
    "\n",
    "def get_image():\n",
    "    # wir brauchen CHW (channel width, height), wir bekommen ein anderes Format \n",
    "    screen = env.render(mode='rgb_array').transpose(\n",
    "        (2,0,1)\n",
    "    )\n",
    "    screen = screen[:, 160:320]\n",
    "    view = 320\n",
    "    cart = cart_pos()\n",
    "    # center the cart\n",
    "    if cart < view // 2:            # floor division --> abgerundet\n",
    "        sliced = slice(view)\n",
    "    elif cart > width - view // 2:\n",
    "        sliced = slice(-1*view, None)\n",
    "    else:\n",
    "        sliced = slice(cart - view // 2, cart + view // 2)\n",
    "    screen = screen[:,:,sliced]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "        \n",
    "#env.reset()\n",
    "#plt.figure()\n",
    "#plt.imshow(get_image().cpu().squeeze(0).permute(1,2,0).numpy(), interpolation='none')\n",
    "#plt.show()\n",
    "\n",
    "# model\n",
    "model = net()\n",
    "#model = model.cuda()\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "mem = Memory(16300)\n",
    "done = 0\n",
    "\n",
    "eps_end = 0.95\n",
    "eps_start = 0.95\n",
    "eps_steps = 150\n",
    "batch_size = 128\n",
    "# for reward\n",
    "gamma = 0.99\n",
    "def get_action(state):\n",
    "    global done\n",
    "    epsilon = random.random()\n",
    "    threshold = (eps_end + eps_start - eps_end) * math.exp(-1. * done / eps_steps)\n",
    "    done = done + 1\n",
    "    if epsilon > threshold:\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1,1)\n",
    "    else:\n",
    "        return LongTensor([[random.randint(0,1)]])\n",
    "\n",
    "# q - learning\n",
    "def train():\n",
    "    if len(mem) < batch_size:\n",
    "        return\n",
    "    x = mem.sample(batch_size)         # (state, action, next_state, reward)\n",
    "    batch = tuple(zip(*x))                 # ((s1,s2, ...), (a1,a2,...), (n1, n2, ...), (r1,r2,...))\n",
    "    non_final = ByteTensor(tuple(map(lambda s: s is not None, batch[2])))\n",
    "    non_final_next = Variable(torch.cat([s for s in batch[2] if s is not None]), volatile=True)\n",
    "    state = Variable(torch.cat(batch[0]))\n",
    "    action = Variable(torch.cat(batch[1]))\n",
    "    reward = Variable(torch.cat(batch[3]))\n",
    "    action_value = model(state).gather(1, action)\n",
    "    # v - value\n",
    "    next_value = Variable(torch.zeros(batch_size).type(FloatTensor))\n",
    "    next_value[non_final] = model(non_final_next).max(1)[0]\n",
    "    next_value.volatile = False\n",
    "    target_action_value = (next_value * gamma) + reward\n",
    "    loss = F.smooth_l1_loss(action_value, target_action_value)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "made_it = []\n",
    "train_duration = 50\n",
    "for i in range(train_duration):\n",
    "    if i % (train_duration / 10) == 0:\n",
    "        print('epoch:', i)\n",
    "    env.reset()\n",
    "    last = get_image()\n",
    "    current = get_image()\n",
    "    state = current - last\n",
    "    for j in count():\n",
    "        action = get_action(state)\n",
    "        _,reward, lost, _ = env.step(int(action[0,0]))\n",
    "        reward = Tensor([reward])\n",
    "        \n",
    "        last = current\n",
    "        current = get_image()\n",
    "        if lost:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = current - last\n",
    "        \n",
    "        mem.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        train()\n",
    "        if lost:\n",
    "            made_it.append(j)\n",
    "            #show()\n",
    "            break\n",
    "        \n",
    "        \n",
    "#env.render(close= True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.figure()\n",
    "plt.plot(made_it)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351a2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
