{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "# read videos\n",
    "from os import listdir\n",
    "import cv2\n",
    "# others\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# SINDy\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "import sindy_utils as sindy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printProgress(epoch, batch_id, loss):\n",
    "    \"\"\"\n",
    "    print progress of the training\n",
    "    epoch: number\n",
    "    batch_id: current batch_id\n",
    "    accuracy:\n",
    "    loss:\n",
    "    \n",
    "    \"\"\"\n",
    "    progress = '='* int((10. * (batch_id+1) / len(train_data)))\n",
    "    progress += '>'\n",
    "    if batch_id == 0:\n",
    "        print('Train Epoche {}: {}% [{}]\\t , loss: {:.6f}'.format(\n",
    "            epoch+1, int(100. * (batch_id+1) / len(train_data)),progress, loss.item()), end='')\n",
    "    else:\n",
    "        print('\\rTrain Epoche {}: {}% [{}]\\t , loss: {:.6f}'.format(\n",
    "            epoch+1, int(100. * (batch_id+1) / len(train_data)),progress, loss.item()), end='', flush = True)\n",
    "        \n",
    "\n",
    "def networkLoss():\n",
    "    \"\"\"\n",
    "    needs:  network['dx'], network['dx_sindy'], network['dz'],  network['dz_sindy'], network['Xi'], network['ae_loss'], \n",
    "            params['loss_weight_decoder'], params['loss_weight_sindy_x'], params['loss_weight_sindy_z'], params['loss_weight_sindy_regularization']\n",
    "\n",
    "    calculate the loss of autoencoder and SINDy combined. loss function of:\n",
    "    \n",
    "     O \\   _________           ________  /  O\n",
    "     .    |         | \\  O  / |        |    .\n",
    "     . -  | phi'(x) | -  O  - | phi(z) | -  .\n",
    "     .    |_________| /  O  \\ |________|    .\n",
    "     O /                                 \\  O\n",
    "    x(t)                z(t)               xa(t)\n",
    "    \n",
    "    ||x-phi(z)||_2^2 + lam1 ||dx - (zGrad phi(z)) Theta(z^T) Xi||_2^2 + lam2 ||dz - Theta(z^T) Xi||_2^2 + lam3 ||Xi||_1\n",
    "        decoder      +                   SINDy in dx                  +         SINDy in dz             +   SINDy sparsity\n",
    "     \n",
    "    dz = xGrad phi'(x) dx = (xGrad z) dx\n",
    "    \n",
    "    network: data of the network\n",
    "        network['dx'], network['dx_sindy'], network['dz'], network['dz_sindy'], network['Xi'], network['ae_loss']\n",
    "    params: hyperparameters\n",
    "    separate_loss: dict of loss\n",
    "        separate_loss['ae_loss'], separate_loss['sindy_x_loss'], separate_loss['sindy_z_loss'], separate_loss['sparse_loss']\n",
    "\n",
    "    return:  summed_total_loss\n",
    "            and saves separate loss in network['...']\n",
    "    \n",
    "    \"\"\"\n",
    "    dx = network['dx']\n",
    "    dx_sindy = network['dx_sindy']\n",
    "    dz = network['dz']\n",
    "    dz_sindy = network['dz_sindy']\n",
    "    Xi_coeff = network['Xi']\n",
    "    ae_loss = float(network['ae_loss'])\n",
    "    sindy_x_loss = float(torch.mean((dx-dx_sindy)**2))\n",
    "    sindy_z_loss = float(torch.mean((dz-dz_sindy)**2))\n",
    "    sparse_loss = float(torch.mean(torch.abs(Xi_coeff)))\n",
    "    \n",
    "    # separate view of each loss\n",
    "    network['ae_loss'] = ae_loss\n",
    "    network['sindy_x_loss'] = sindy_x_loss\n",
    "    network['sindy_z_loss'] = sindy_z_loss\n",
    "    network['sparse_loss'] = sparse_loss\n",
    "    \n",
    "    tot_loss = (params['loss_weight_decoder'] * ae_loss\n",
    "                + params['loss_weight_sindy_x'] * sindy_x_loss \n",
    "                + params['loss_weight_sindy_z'] * sindy_z_loss\n",
    "                + params['loss_weight_sindy_regularization'] * sparse_loss)\n",
    "                                                                                        \n",
    "    return tot_loss\n",
    "\n",
    "\n",
    "def calculateSindy():\n",
    "    '''\n",
    "    needs: network['z'], network['dz'], params['poly_order'], params['include_sine'], params['sindy_threshold']\n",
    "    Calculate Theta(z) and regress to get Xi and save it in network['...']\n",
    "\n",
    "    return: dz prediction\n",
    "    \n",
    "    '''\n",
    "    z = network['z'].cpu().detach().numpy()\n",
    "    dz = network['dz'].cpu().detach().numpy()\n",
    "    \n",
    "    network['Theta'] = torch.from_numpy(sindy.sindy_library(z, params['poly_order'], include_sine=params['include_sine']))\n",
    "    network['Xi'] = torch.from_numpy(sindy.sindy_fit(network['Theta'], dz, params['sindy_threshold']))\n",
    "    dz_predict = torch.matmul(network['Theta'],network['Xi']).cuda()\n",
    "    \n",
    "    return dz_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "# autoencoder settings\n",
    "params['number_epoch_ae'] = 5001                         # number of epochs only autoencoder\n",
    "params['number_epoch_sindy'] = 1000\n",
    "params['z_dim'] = 2                                     # number of coordinates for SINDy\n",
    "params['batch_size'] = 16                                # batch size\n",
    "params['lr_rate'] = 1e-5                                 # learning rate\n",
    "params['weight_decay'] = 1e-8                               # weight decay for NN optimizer\n",
    "\n",
    "# loss function weighting \n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_x'] = 1e-3\n",
    "params['loss_weight_sindy_z'] = 0\n",
    "params['loss_weight_sindy_regularization'] = 1e-6\n",
    "\n",
    "# SINDy parameters\n",
    "params['sindy_threshold'] = 0.5 \n",
    "params['poly_order'] = 4\n",
    "params['include_sine'] = False\n",
    "\n",
    "# video processing\n",
    "path_train = '../../Videos/train/'\n",
    "path_autoencoder = 'results/v4_lre-5_z5_poly4/Ae_600epoch_bs16_lr1e-5_z5_sindth0-5_poly5.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available:  True\n",
      "Read training data: Cu_2_Trim_low.mov\n",
      "train data:  38 16 3 404 720\n",
      "index of new videos:  [0]\n",
      "train data reading done!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36036/2440629927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mwhereInData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_idxOfNewVideo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoose\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# check if there are more than 3 batches of frames available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_idxOfNewVideo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoose\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_idxOfNewVideo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoose\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0melement1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwhereInData\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0melement2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwhereInData\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('cuda available: ', torch.cuda.is_available())\n",
    "#print('cuda memory', torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# read the train videos in random order\n",
    "file_names = []\n",
    "# for f in listdir(path_train):\n",
    "#     if f != 'high_res':\n",
    "#         file_names.append(f)\n",
    "\n",
    "# random.shuffle(file_names)\n",
    "\n",
    "# for testing purpose, only take one video\n",
    "file_names.append('Cu_2_Trim_low.mov')\n",
    "\n",
    "# define transform to tensor and resize to 1080x1920, 720x404 (16:9)\n",
    "# pictures are 16:9 --> 1080x1920, 900x1600, 720x1280, 576x1024, 540x960: 500k pixel, 360x640, 272x480\n",
    "# normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])    # normalize around mean with sigma (std)\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((1080, 1920))])\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# read data to list and transform to tensor\n",
    "train_data_tmp = []\n",
    "train_data = []\n",
    "train_idxOfNewVideo = []\n",
    "count = 0\n",
    "for f in file_names:\n",
    "    # if count == 3:\n",
    "    #     break\n",
    "    count += 1\n",
    "    train_idxOfNewVideo.append(len(train_data))\n",
    "    vidcap = cv2.VideoCapture(path_train + f)\n",
    "    success,imgR = vidcap.read()\n",
    "    print('Read training data:',f)\n",
    "    while success:\n",
    "        imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2RGB)\n",
    "        imgR_tensor = transform(imgR)\n",
    "        train_data_tmp.append(imgR_tensor)\n",
    "        success,imgR = vidcap.read()\n",
    "        # make a batch\n",
    "        if len(train_data_tmp) >= params['batch_size']:\n",
    "            train_data.append(torch.stack(train_data_tmp))\n",
    "            train_data_tmp = []\n",
    "    \n",
    "    print('train data: ', len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]), len(train_data[0][0][0][0]))\n",
    "\n",
    "vidcap.release()\n",
    "\n",
    "print('index of new videos: ', train_idxOfNewVideo)\n",
    "print('train data reading done!')\n",
    "\n",
    "\n",
    "# split into validation and training set\n",
    "validation_data = []\n",
    "# take 10% of training set batches to validation set, take always two\n",
    "nbr_batch = int(len(train_data)*0.1 / 2)\n",
    "# take first two frames of a video --> goal: no interruption of the video\n",
    "for i in range(0,nbr_batch):\n",
    "    # choose position of train_idxOfNewVideo\n",
    "    choose = random.randint(0, len(train_idxOfNewVideo)-1)\n",
    "    whereInData = train_idxOfNewVideo[choose]\n",
    "    # check if there are more than 3 batches of frames available\n",
    "    if (train_idxOfNewVideo[choose+1] - train_idxOfNewVideo[choose]) > 3:\n",
    "        element1 = train_data[whereInData]\n",
    "        element2 = train_data[whereInData+1]\n",
    "        validation_data.append(element1)\n",
    "        validation_data.append(element2)\n",
    "        train_data.pop(whereInData+1)\n",
    "        train_data.pop(whereInData)\n",
    "        # adapt index where new videos start in train data\n",
    "        for j in range(choose+1, len(train_idxOfNewVideo)):\n",
    "            train_idxOfNewVideo[j] -= 2\n",
    "\n",
    "print('validation data construction done: ', len(validation_data), len(validation_data[0]), len(validation_data[0][0]), len(validation_data[0][0][0]))\n",
    "print('train data: ', len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]), len(train_data[0][0][0][0]))\n",
    "print('index of new videos: ', train_idxOfNewVideo)\n",
    "\n",
    "\n",
    "# delete not used objects\n",
    "del train_data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: Ac_timelapse_sunrise_low.mov\n",
      "Read: Cb_2_low.mov\n",
      "Read: St_Fabio1_low.mov\n",
      "test data reading done:  889 3 404 720\n"
     ]
    }
   ],
   "source": [
    "# split into test set\n",
    "test_data = []\n",
    "# take 10% of training set batches to test set, take always two\n",
    "nbr_batch = int(len(train_data)*0.1 / 4)\n",
    "# take first two frames of a video --> goal: no interruption of the video\n",
    "for i in range(0,nbr_batch):\n",
    "    # choose position of train_idxOfNewVideo\n",
    "    choose = random.randint(0, len(train_idxOfNewVideo)-1)\n",
    "    whereInData = train_idxOfNewVideo[choose]\n",
    "    # check if there are more than 3 batches of frames available\n",
    "    if (train_idxOfNewVideo[choose+1] - train_idxOfNewVideo[choose]) > 5:\n",
    "        element1 = train_data[whereInData]\n",
    "        element2 = train_data[whereInData+1]\n",
    "        element3 = train_data[whereInData+2]\n",
    "        element4 = train_data[whereInData+3]\n",
    "        test_data.append(element1)\n",
    "        test_data.append(element2)\n",
    "        test_data.append(element3)\n",
    "        test_data.append(element4)\n",
    "        train_data.pop(whereInData+3)\n",
    "        train_data.pop(whereInData+2)\n",
    "        train_data.pop(whereInData+1)\n",
    "        train_data.pop(whereInData)\n",
    "        # adapt index where new videos start in train data\n",
    "        for j in range(choose+1, len(train_idxOfNewVideo)):\n",
    "            train_idxOfNewVideo[j] -= 2\n",
    "\n",
    "print('test data construction done: ', len(test_data), len(test_data[0]), len(test_data[0][0]), len(test_data[0][0][0]))\n",
    "print('train data: ', len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]), len(train_data[0][0][0][0]))\n",
    "\n",
    "# save training, validation and test data\n",
    "name_path = 'results/v5/data/'\n",
    "torch.save(train_data, name_path + 'train_data.pt')\n",
    "torch.save(validation_data, name_path + 'validation_data.pt')\n",
    "torch.save(test_data, name_path + 'test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot some frames and test prints\n",
    "print(torch.min(train_data[0]), torch.max(train_data[0]))\n",
    "print(train_data[0].permute(3,2,1,0).size())\n",
    "\n",
    "# plot first frame per batch\n",
    "for i in range(len(train_data)):\n",
    "    if i%3 == 0:\n",
    "        plt.figure()\n",
    "    imgP = train_data[i][0].permute(1,2,0).detach().numpy()\n",
    "    plt.subplot(1,3, i%3 + 1)\n",
    "    plt.imshow(imgP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINDy autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__() \n",
    "        self.encode = nn.Sequential(\n",
    "            # encoder: N, 3, 404, 720\n",
    "            nn.Conv2d(3, 16, 2), # N, 16, 403, 719\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 2), # N, 32, 402, 718\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,3), stride=(2,3)), # N, 32, 201, 239              -- pool --\n",
    "            nn.Conv2d(32, 64, 4), # N, 64, 198, 236\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 96, 4), # N, 96, 195, 233\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2), # N, 96, 97, 116                       -- pool --\n",
    "            nn.Conv2d(96, 128, 5), # N, 128, 93, 112\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 150, 5, stride=2, padding=1), # N, 150, 46, 55\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,stride=2), # N, 150, 23, 27                        -- pool --\n",
    "            nn.Conv2d(150, 200, 9, stride=2), # N, 200, 8, 10\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # TODO: Dopoutlayers --> in the end to fine tune\n",
    "        self.fc1 = nn.Linear(200*8*10,params['z_dim'])\n",
    "        # Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
    "        # Input [-1, +1] -> use nn.Tanh    \n",
    "        \n",
    "        # note: encoder and decoder are not symmetric\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.ConvTranspose2d(200, 150, 4), # N, 150, 11, 13\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(150, 128, 5, stride=(2,3), padding=(2,2), output_padding=(0,2)), # N, 128, 21, 39\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 96, 4, stride=2, padding=(1,0)), # N, 96, 42, 80\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(96, 64, 8), # N, 64, 49, 87\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 8, stride=2, padding=(2,1), output_padding=(0,1)), # N, 32, 100, 179\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 5, stride=2, padding=1), # N, 16, 201, 359\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 5, stride=2, padding=1, output_padding=(1,1)), # N, 3, 404, 720\n",
    "            nn.ReLU()\n",
    "        )   \n",
    "        \n",
    "        self.fc2 = nn.Linear(params['z_dim'], 200*8*10)\n",
    "\n",
    "    def forward(self, x, z, mode):\n",
    "        '''\n",
    "        x: input for encoder\n",
    "        z: input for decoder\n",
    "        mode: \n",
    "            'train' -> use encoded for decoder\n",
    "            'test'  -> feed z in an get decoded\n",
    "        \n",
    "        '''\n",
    "        if mode == 'train':\n",
    "            encoded = self.encode(x)\n",
    "            encoded = encoded.view(-1,200*8*10)\n",
    "            encoded = F.sigmoid(self.fc1(encoded))        # squash network into [0,1]\n",
    "\n",
    "            decoded = self.fc2(encoded)\n",
    "            decoded = decoded.view(-1,200,8,10)\n",
    "            decoded = self.decode(decoded)\n",
    "        elif mode == 'test':\n",
    "            encoded = torch.zeros(1)\n",
    "\n",
    "            decoded = self.fc2(z)\n",
    "            decoded = decoded.view(-1,200,8,10)\n",
    "            decoded = self.decode(decoded)\n",
    "        else:\n",
    "            print(mode, 'is not an adequate in the forward path')\n",
    "\n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36036/3337944517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;31m# first train only autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number_epoch_ae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'autoencoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in phase autoencoder done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'autoencoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36036/3337944517.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, phase)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_tensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mimg_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[1;31m#.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mencode_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecon_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ae_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'combined_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ae_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36036/719400714.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, z, mode)\u001b[0m\n\u001b[0;32m     59\u001b[0m         '''\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# squash network into [0,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "# # load model\n",
    "# if os.path.isfile(path_autoencoder):\n",
    "#     autoencoder = torch.load(path_autoencoder)\n",
    "#     autoencoder = autoencoder.cpu()\n",
    "#     print('loaded autoencoder', path_autoencoder)\n",
    "# else:\n",
    "#     autoencoder = Autoencoder()\n",
    "#     autoencoder = autoencoder.cpu()        # or .cuda()\n",
    "#     print('loaded new autoencoder')\n",
    "\n",
    "\n",
    "# to save network data and initialize loss values\n",
    "network = {}\n",
    "network['combined_loss'] = 0\n",
    "network['sindy_x_loss'] = 0\n",
    "network['sindy_z_loss'] = 0\n",
    "network['sparse_loss'] = 0\n",
    "\n",
    "# training function\n",
    "def train(epoch, phase):\n",
    "    '''\n",
    "    training function for the autoencoder: \n",
    "        use train_idxOfNewVideo to check if the current batch correspond to a new video\n",
    "\n",
    "    epoch: current epoch of learning\n",
    "    phase: 'autoencoder', 'sindy' --> first train only auto encoder (pretrain), then with the sindy loss terms\n",
    "\n",
    "    '''\n",
    "    # train only with autoencoder\n",
    "    if phase == 'autoencoder':\n",
    "        for batch_id, img_tensor in enumerate(train_data):\n",
    "            img_tensor = img_tensor#.cuda()\n",
    "            encode_tensor, recon_tensor = autoencoder(img_tensor, 0, mode='train')\n",
    "            network['ae_loss'] = criterion(recon_tensor, img_tensor)\n",
    "            network['combined_loss'] = network['ae_loss']\n",
    "            combined_loss = network['ae_loss']\n",
    "\n",
    "            # optimization step\n",
    "            optimizer.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # tensorboard\n",
    "            writer.add_scalar('Training loss per batch in autoencoder phase', combined_loss, global_step=step_train)\n",
    "            writer.add_histogram('fc1', autoencoder.fc1.weight)\n",
    "            step_train += 1\n",
    "\n",
    "            # printProgress(epoch, batch_id, combined_loss)\n",
    "            img_tensor = img_tensor.detach()\n",
    "    \n",
    "    # train with autoencoder and sindy\n",
    "    elif phase == 'sindy':\n",
    "        pos = 0\n",
    "        for batch_id, img_tensor in enumerate(train_data):\n",
    "            img_tensor = img_tensor#.cuda()\n",
    "            encode_tensor, recon_tensor = autoencoder(img_tensor, 0, mode='train')\n",
    "            network['ae_loss'] = criterion(recon_tensor, img_tensor)\n",
    "            \n",
    "            # x, z is current batch_id, dx, dz is next one (in else we take dz as current and compare with x from before, the excite to current step)\n",
    "            if batch_id == train_idxOfNewVideo[pos]:\n",
    "                pos += 1\n",
    "                combined_loss = network['ae_loss']       \n",
    "                network['z'] = encode_tensor#.float()\n",
    "            else:\n",
    "                network['dx'] = img_tensor#.float()\n",
    "                network['dz'] = encode_tensor#.float()\n",
    "                network['dz_sindy'] = calculateSindy().float()\n",
    "                _, network['dx_sindy'] = autoencoder(0, network['dz_sindy'], mode='test')\n",
    "                combined_loss = networkLoss()            # total loss with SINDy\n",
    "                # now advance one step\n",
    "                network['z'] = network['dz']\n",
    "                    \n",
    "            # optimization and backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # tensorboard\n",
    "            writer.add_scalar('Training loss per batch in sindy phase', combined_loss, global_step=step_train)\n",
    "            writer.add_histogram('fc1', autoencoder.fc1.weight)\n",
    "            step_train += 1\n",
    "\n",
    "            # printProgress(epoch, batch_id, combined_loss)\n",
    "            img_tensor = img_tensor.detach()\n",
    "    else:\n",
    "        print('No such training phase available:', phase)\n",
    "\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    # hyperparametering with the last losses of the epoch\n",
    "    writer.add_hparams({'lr': lr_rate, 'zDimension': dimZ}, \n",
    "           {'auto encoder loss': network['ae_loss'],'combined loss': network['combined_loss'], 'sindy x loss': network['sindy_x_loss'], 'sparsity loss': network['sparse_loss']})\n",
    "    \n",
    "    # delete from cuda\n",
    "    del encode_tensor\n",
    "    del recon_tensor\n",
    "\n",
    "\n",
    "# evaluation function\n",
    "def evaluate(phase):\n",
    "    '''\n",
    "    evaluation of the training by it's loss\n",
    "\n",
    "    '''\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # train only with autoencoder\n",
    "    if phase == 'autoencoder':\n",
    "        evaluated_combined_loss = 0\n",
    "        for i, img_tensor in enumerate(validation_data):\n",
    "            img_tensor = img_tensor#.cuda()\n",
    "            encode_eval_tensor, recon_eval_tensor = autoencoder(img_tensor, 0, mode='train')\n",
    "            network['ae_loss'] = criterion(recon_eval_tensor, img_tensor)\n",
    "            evaluated_combined_loss += network['ae_loss']\n",
    "\n",
    "        # append average loss of this epoch\n",
    "        evaluated_combined_loss_perData = evaluated_combined_loss/len(validation_data)\n",
    "        writer.add_scalar('Evaluation combined loss per epoch in autoencoder phase', evaluated_combined_loss_perData, global_step=step_eval)\n",
    "        step_eval += 1\n",
    "\n",
    "    # train with autoencoder and sindy\n",
    "    elif phase == 'sindy':\n",
    "        evaluated_combined_loss = 0\n",
    "        evaluated_sindy_loss = 0\n",
    "        for i, img_tensor in enumerate(validation_data):\n",
    "            img_tensor = img_tensor#.cuda()\n",
    "            encode_eval_tensor, recon_eval_tensor = autoencoder(img_tensor, 0, mode='train')\n",
    "            network['ae_loss'] = criterion(recon_eval_tensor, img_tensor)\n",
    "\n",
    "            # first batch\n",
    "            if i % 2 == 0:\n",
    "                # x, z are at the current time\n",
    "                network['z'] = encode_eval_tensor#.float()\n",
    "                \n",
    "            # second batch, evaluation\n",
    "            else:\n",
    "                network['dx'] = img_tensor#.float()\n",
    "                network['dz'] = encode_eval_tensor#.float()\n",
    "                eval_theta = torch.from_numpy(sindy.sindy_library(network['z'].cpu().detach().numpy(), params['poly_order'], include_sine=params['include_sine']))\n",
    "                network['dz_sindy'] = torch.matmul(eval_theta,network['Xi']).float()#.cuda()\n",
    "                \n",
    "                _, network['dx_sindy'] = autoencoder(0, network['dz_sindy'], mode='test')\n",
    "                combined_loss = networkLoss()            # total loss with SINDy\n",
    "\n",
    "                # loss\n",
    "                evaluated_combined_loss += combined_loss\n",
    "                evaluated_sindy_loss += network['sindy_x_loss']\n",
    "        \n",
    "        # append average loss of this epoch\n",
    "        evaluated_combined_loss_perData = evaluated_combined_loss/len(validation_data)*2\n",
    "        evaluated_sindy_loss_perData = evaluated_sindy_loss/len(validation_data)*2\n",
    "        writer.add_scalar('Evaluation combined loss per epoch in sindy phase', evaluated_combined_loss_perData, global_step=step_eval)\n",
    "        writer.add_scalar('Evaluation sindy x loss per epoch in sindy phase', evaluated_sindy_loss_perData, global_step=step_eval)\n",
    "        step_eval += 1\n",
    "    else:\n",
    "        print('No such evaluation phase available:', phase)\n",
    "\n",
    "\n",
    "    del encode_eval_tensor\n",
    "    del recon_eval_tensor\n",
    "\n",
    "\n",
    "\n",
    "# print to tensorboard and hyperparameter search\n",
    "# lr_rate_arr = [0.01, 0.001, 0.0001, 0.00001]\n",
    "# dim_z_arr = [1, 2, 3, 5, 10, 20]\n",
    "lr_rate_arr = [params['lr_rate']]\n",
    "dim_z_arr = [params['z_dim']]\n",
    "\n",
    "for lr_rate in lr_rate_arr:\n",
    "    params['lr_rate'] = lr_rate\n",
    "    for dimZ in dim_z_arr:\n",
    "        params['z_dim'] = dimZ\n",
    "        writer = SummaryWriter(f'runs/v5Tensorboard/trainLoss_LR{lr_rate}_dimZ{dimZ}')\n",
    "        step_train = 0\n",
    "        step_eval = 0\n",
    "\n",
    "        # load new network\n",
    "        autoencoder = Autoencoder()\n",
    "        autoencoder = autoencoder.cuda()\n",
    "        # optimization technique\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=params['lr_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "        # epoch loop\n",
    "        for epoch in range(params['number_epoch_ae'] + params['number_epoch_sindy']):\n",
    "            # first train only autoencoder\n",
    "            if epoch < params['number_epoch_ae']:\n",
    "                train(epoch, phase='autoencoder')\n",
    "                print('train epoch', epoch, 'in phase autoencoder done')\n",
    "                evaluate(phase='autoencoder')\n",
    "                print('evaluate epoch', epoch, 'in phase autoencoder done')\n",
    "            # afterwards train network with sindy\n",
    "            else:\n",
    "                train(epoch, phase='sindy')\n",
    "                print('train epoch', epoch, 'in phase sindy done')\n",
    "                evaluate(phase='sindy')\n",
    "                print('evaluate epoch', epoch, 'in phase sindy done')\n",
    "\n",
    "            # save model every 1000 epoch\n",
    "            if epoch % 1000 == 0:\n",
    "                name_Ae = 'results/v5/Ae_' + str(epoch) + 'epoch_bs16_lr{lr_rate}_z{dimZ}_sindt05_poly5.pt'\n",
    "                name_Xi = 'results/v5/Xi_' + str(epoch) + 'epoch_bs16_lr{lr_rate}_z{dimZ}_sindth05_poly5.pt'\n",
    "                torch.save(autoencoder, name_Ae)\n",
    "                torch.save(network['Xi'], name_Xi)\n",
    "                print('saved model in epoch', epoch)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINDy prediction --> split up model that I can plug in z\n",
    "t = np.arange(0,20,.01)\n",
    "z_sim = sindy.sindy_simulate(network['z'], t, network['Xi'], params['poly_order'], params['include_sine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "print(network['Theta'].shape, network['Xi'].shape)\n",
    "#print(network['Xi'])\n",
    "\n",
    "print('reconstruction loss', categorie_loss[0],'\\n sindy x loss', categorie_loss[1], \n",
    "      '\\n sindy z loss', categorie_loss[2], '\\n Xi sparsity loss', categorie_loss[3])\n",
    "print(network['ae_loss'])\n",
    "\n",
    "for i in range(0, params['number_epoch']):\n",
    "    plt.figure()\n",
    "    realImg = outputs[i][1].permute(0,2,3,1).detach().numpy()\n",
    "    reconImg = outputs[i][2].permute(0,2,3,1).detach().numpy()\n",
    "    for i, item in enumerate(realImg):\n",
    "        if i >=4: break\n",
    "        plt.subplot(2,4,i+1)\n",
    "        plt.imshow(item)\n",
    "    for i, item in enumerate(reconImg):\n",
    "        if i >=4: break\n",
    "        plt.subplot(2,4,4+i+1)\n",
    "        plt.imshow(item)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6]]\n",
      "[[5, 6], [1, 2]]\n",
      "2 2\n",
      "[[3, 3], [4, 2], [5, 34]]\n",
      "[[3, 3], [4, 2], [5, 0]]\n",
      "[[3, 3], [4, 2], [5, 0, [10, 9]]]\n",
      "[[]]\n",
      "6 16 2 5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sigmoid(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36036/1490953444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mtestSigmoid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSigmoid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sigmoid(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "## for testing\n",
    "#Xi = np.ones((5,2))\n",
    "#Xi[1,1] = 4\n",
    "#Xi[3,0] = 0.01\n",
    "#Xi[2,1] = 1.2\n",
    "#Xi[4,1] = 0.09\n",
    "#print(Xi)\n",
    "#small_inds = (np.abs(Xi) < 0.1)\n",
    "#print(small_inds)\n",
    "#Xi[small_inds] = 0\n",
    "#print(Xi)\n",
    "\n",
    "haList = [[]]\n",
    "haList[0].append(5)\n",
    "haList[0].append(6)\n",
    "print(haList)\n",
    "haList.append([1])\n",
    "haList[1].append(2)\n",
    "print(haList)\n",
    "\n",
    "print(len(haList), len(haList[0]))\n",
    "haList = [[3,3],[4,2],[5,34]]\n",
    "print(haList)\n",
    "haList[2][1] = 0\n",
    "print(haList)\n",
    "haList[2].append([10,9])\n",
    "print(haList)\n",
    "\n",
    "\n",
    "\n",
    "# only use one list to read data --> less memory and faster!\n",
    "readTest = [[]]\n",
    "readTest_tensor = torch.ones((2,5))\n",
    "readTest_tensor2 = torch.zeros((2,5))\n",
    "batch_id_local = 0\n",
    "print(readTest)\n",
    "for i in range(100):\n",
    "    #print(batch_id_local, i)\n",
    "    if i % 16 == 0 and i != 0:\n",
    "        batch_id_local += 1\n",
    "        readTest.append([readTest_tensor2])\n",
    "    else:\n",
    "        readTest[batch_id_local].append(readTest_tensor)\n",
    "\n",
    "if len(readTest[batch_id_local]) < batch_id_local:\n",
    "    readTest.remove(readTest[batch_id_local])\n",
    "\n",
    "print(len(readTest), len(readTest[0]),len(readTest[0][0]),len(readTest[0][0][0]))\n",
    "\n",
    "testTensor = torch.rand((1,4))\n",
    "testSigmoid = torch.sigmoid(10)\n",
    "print(testSigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
