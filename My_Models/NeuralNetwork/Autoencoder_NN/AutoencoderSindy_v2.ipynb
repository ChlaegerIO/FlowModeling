{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "# read videos\n",
    "import pylab          # play video\n",
    "from os import listdir\n",
    "import cv2\n",
    "# others\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# SINDy\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "import sindy_utils as sindy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print progress, possibly faster than progress bar\n",
    "def printProgress(epoch, batch_id, accuracy, loss):\n",
    "    \"\"\"\n",
    "    print progress of the training\n",
    "    epoch: number\n",
    "    batch_id: current batch_id\n",
    "    accuracy:\n",
    "    loss:\n",
    "    \n",
    "    \"\"\"\n",
    "    progress = '='* int((10. * (batch_id+1) / len(train_data)))\n",
    "    progress += '>'\n",
    "    if batch_id == 0:\n",
    "        print('Train Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch+1, int(100. * (batch_id+1) / len(train_data)),progress, accuracy, loss.item()), end='')\n",
    "    else:\n",
    "        print('\\rTrain Epoche {}: {}% [{}]\\t accuracy: {:.6f}, loss: {:.6f}'.format(\n",
    "            epoch+1, int(100. * (batch_id+1) / len(train_data)),progress, accuracy, loss.item()), end='', flush = True)\n",
    "        \n",
    "\n",
    "def calculateLoss(network, params):\n",
    "    \"\"\"\n",
    "    calculate the loss of autoencoder and SINDy combined. loss function of:\n",
    "    \n",
    "     O \\   _________           ________  /  O\n",
    "     .    |         | \\  O  / |        |    .\n",
    "     . -  | phi'(x) | -  O  - | phi(z) | -  .\n",
    "     .    |_________| /  O  \\ |________|    .\n",
    "     O /                                 \\  O\n",
    "    x(t)                z(t)               xa(t)\n",
    "    \n",
    "    ||x-phi(z)||_2^2 + lam1 ||dx - (zGrad phi(z)) Theta(z^T) Xi||_2^2 + lam2 ||dz - Theta(z^T) Xi||_2^2 + lam3 ||Xi||_1\n",
    "        decoder      +                   SINDy in dx                  +         SINDy in dz             +   SINDy sparsity\n",
    "     \n",
    "    dz = xGrad phi'(x) dx = (xGrad z) dx\n",
    "    \n",
    "    network: data of the network\n",
    "    params: hyperparameters\n",
    "    \n",
    "    \"\"\"\n",
    "    dx = network['dx']\n",
    "    dx_decode = network['dx_decode']\n",
    "    dz = network['dz']\n",
    "    dz_predict = network['dz_predict']\n",
    "    Xi_coeff = network['Xi']\n",
    "    rec_loss = network['rec_loss']\n",
    "    sindy_x_loss = torch.mean((dx - dx_decode)**2)\n",
    "    sindy_z_loss = torch.mean((dz - dz_predict)**2)\n",
    "    sparse_loss = torch.mean(torch.abs(Xi_coeff))\n",
    "    \n",
    "    # separate view of each loss\n",
    "    separate_loss = []\n",
    "    separate_loss.append(rec_loss)\n",
    "    separate_loss.append(sindy_x_loss)\n",
    "    separate_loss.append(sindy_z_loss)\n",
    "    separate_loss.append(sparse_loss)\n",
    "    \n",
    "    tot_loss = (params['loss_weight_decoder'] * rec_loss\n",
    "                + params['loss_weight_sindy_x'] * sindy_x_loss \n",
    "                + params['loss_weight_sindy_z'] * sindy_z_loss\n",
    "                + params['loss_weight_sindy_regularization'] * sparse_loss)\n",
    "                                                                                        \n",
    "    return tot_loss, separate_loss\n",
    "\n",
    "\n",
    "def calculateSindy(network, params):\n",
    "    '''\n",
    "    Calculate Theta(z), Xi and dz\n",
    "    \n",
    "    '''\n",
    "    z = network['z'].detach().numpy()\n",
    "    dz = network['dz'].detach().numpy()\n",
    "    \n",
    "    network['Theta'] = torch.from_numpy(sindy.sindy_library(z, params['poly_order'], include_sine=params['include_sine']))\n",
    "    network['Xi'] = torch.from_numpy(sindy.sindy_fit(network['Theta'], dz, params['sindy_threshold']))\n",
    "    dz_predict = np.dot(network['Theta'],network['Xi'])\n",
    "    \n",
    "    return torch.from_numpy(dz_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['number_epoch'] = 2                               # number of epochs\n",
    "params['z_dim'] = 10                                     # number of coordinates for SINDy\n",
    "params['batch_size'] = 8                                # batch size\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "# SINDy parameters\n",
    "params['sindy_threshold'] = 0.5 \n",
    "params['poly_order'] = 2\n",
    "params['include_sine'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: St_Oct_low.mp4\n",
      "81 8 3 1080 1920\n"
     ]
    }
   ],
   "source": [
    "# read the videos in random order\n",
    "file_names = []\n",
    "for f in listdir('../../Videos/train/'):\n",
    "    file_names.append(f)\n",
    "\n",
    "random.shuffle(file_names)\n",
    "\n",
    "# define transform to tensor and resize to 1080x1920\n",
    "normalize = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])    # normalize around mean with sigma (std)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((1080, 1920))])\n",
    "\n",
    "# read data to list and transform to tensor\n",
    "train_data_tmp = []\n",
    "train_data = []\n",
    "train_idxOfNewVideo = []\n",
    "count = 0\n",
    "for f in file_names:\n",
    "    # just for testing (save time)\n",
    "    if count == 1:\n",
    "        break\n",
    "    count += 1\n",
    "    vidcap = cv2.VideoCapture('../../Videos/train/' + f)\n",
    "    success,imgR = vidcap.read()\n",
    "    print('Read:',f)\n",
    "    while success:\n",
    "        imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2RGB)\n",
    "        imgR_tensor = transform(imgR)\n",
    "        train_data_tmp.append(imgR_tensor)\n",
    "        success,imgR = vidcap.read()\n",
    "        # make a batch\n",
    "        if len(train_data_tmp) >= params['batch_size']:\n",
    "            train_data.append(torch.stack(train_data_tmp))\n",
    "            train_data_tmp = []\n",
    "    train_idxOfNewVideo.append(len(train_data))\n",
    "    print(len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]), len(train_data[0][0][0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]), len(train_data[0][0][0][0]))\n",
    "\n",
    "# split into validation and training set\n",
    "validation_data = []\n",
    "# take 10% of training set batches to validation set\n",
    "nbr_batch = int(len(train_data)*0.1)\n",
    "print(nbr_batch)\n",
    "for i in range(0,nbr_batch):\n",
    "    choose = random.randint(0, len(train_data)-1)\n",
    "    element = train_data[choose]\n",
    "    validation_data.append(element)\n",
    "    train_data.remove(element)\n",
    "    print(len(validation_data), len(validation_data[0]), len(validation_data[0][0]), len(validation_data[0][0][0]), len(validation_data[0][0][0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.min(train_data[0]), torch.max(train_data[0]))\n",
    "print(train_data[0].permute(3,2,1,0).size())\n",
    "\n",
    "# plot first frame per batch\n",
    "for i in range(len(train_data)):\n",
    "    if i%3 == 0:\n",
    "        plt.figure()\n",
    "    imgP = train_data[i][0].permute(1,2,0).detach().numpy()\n",
    "    plt.subplot(1,3, i%3 + 1)\n",
    "    plt.imshow(imgP)\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINDy autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()               \n",
    "        # encoder: N, 3, 1080, 1920\n",
    "        self.conv1_e = nn.Conv2d(3, 16, 5) # N, 16, 1076, 1916\n",
    "        self.conv2_e = nn.Conv2d(16, 32, 5) # N, 32, 1072, 1912\n",
    "        self.pool1_e = nn.MaxPool2d((2,3), stride=(2,3)) # N, 32, 536, 637              -- pool --\n",
    "        self.conv3_e = nn.Conv2d(32, 64, 8, stride=2, padding=1) # N, 64, 266, 316\n",
    "        self.conv4_e = nn.Conv2d(64, 96, 7) # N, 96, 260, 310\n",
    "        self.pool2_e = nn.MaxPool2d(2, stride=2) # N, 96, 130, 155                      -- pool --\n",
    "        self.conv5_e = nn.Conv2d(96, 128, 11) # N, 128, 120, 145\n",
    "        self.conv6_e = nn.Conv2d(128, 150, 4, stride=2, padding=1) # N, 150, 60, 72\n",
    "        self.conv7_e = nn.Conv2d(150, 200, 11) # N, 200, 50, 62\n",
    "        self.pool3_e = nn.MaxPool2d(2,stride=2) # N, 200, 25, 31                        -- pool --\n",
    "        self.conv8_e = nn.Conv2d(200, 200, 9, stride=2, padding=1) # N, 200, 10, 13\n",
    "        self.conv9_e = nn.Conv2d(200, 200, 10) # N, 200, 1, 4\n",
    "        self.fc1 = nn.Linear(200*4,params['z_dim'])\n",
    "        # Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
    "        # Input [-1, +1] -> use nn.Tanh    \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        encoded = F.relu(self.conv1_e(x))\n",
    "        encoded = F.relu(self.conv2_e(encoded))\n",
    "        encoded = F.relu(self.pool1_e(encoded))\n",
    "        encoded = F.relu(self.conv3_e(encoded))\n",
    "        encoded = F.relu(self.conv4_e(encoded))\n",
    "        encoded = F.relu(self.pool2_e(encoded))\n",
    "        encoded = F.relu(self.conv5_e(encoded))\n",
    "        encoded = F.relu(self.conv6_e(encoded))\n",
    "        encoded = F.relu(self.conv7_e(encoded))\n",
    "        encoded = F.relu(self.pool3_e(encoded))\n",
    "        encoded = F.relu(self.conv8_e(encoded))\n",
    "        encoded = F.relu(self.conv9_e(encoded))\n",
    "\n",
    "        #print(encoded.size())\n",
    "        encoded = encoded.view(-1,200*4)\n",
    "        encoded = self.fc1(encoded)\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "# decoder architecture\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()               \n",
    "        # decoder: N, 200, 1, 4\n",
    "        self.fc2 = nn.Linear(params['z_dim'], 200*4)\n",
    "        self.convT9_d = nn.ConvTranspose2d(200, 200, 4) # N, 200, 4, 7\n",
    "        self.convT8_d = nn.ConvTranspose2d(200, 200, 9, stride=2, padding=2, output_padding=(1,0)) # N, 200, 12, 17\n",
    "        self.convT7_d = nn.ConvTranspose2d(200, 150, 8, stride=4, padding=(0,1), output_padding=(3,0)) # N, 150, 55, 70\n",
    "        self.convT6_d = nn.ConvTranspose2d(150, 128, 8) # N, 128, 62, 77\n",
    "        self.convT5_d = nn.ConvTranspose2d(128, 96, 6, stride=(2,3), padding=1) # N, 96, 126, 232\n",
    "        self.convT4_d = nn.ConvTranspose2d(96, 64, 7) # N, 64, 132, 238\n",
    "        self.convT3_d = nn.ConvTranspose2d(64, 32, 8, stride=2, padding=(1,2), output_padding=(1,1)) # N, 32, 296, 479\n",
    "        self.convT2_d = nn.ConvTranspose2d(32, 16, 5, stride=2, padding=1) # N, 16, 539, 959\n",
    "        self.convT1_d = nn.ConvTranspose2d(16, 3, 5, stride=2, padding=1, output_padding=(1,1)) # N, 3, 1080, 1920\n",
    "\n",
    "    def forward(self, x):\n",
    "        decoded = self.fc2(x)\n",
    "        decoded = decoded.view(-1,200,1,4)\n",
    "        decoded = F.relu(self.convT9_d(decoded))\n",
    "        print(decoded.size())\n",
    "        decoded = F.relu(self.convT8_d(decoded))                                    \n",
    "        print(decoded.size())\n",
    "        decoded = F.relu(self.convT7_d(decoded))\n",
    "        print(decoded.size())\n",
    "        decoded = F.relu(self.convT6_d(decoded))\n",
    "        print(decoded.size())\n",
    "        decoded = F.relu(self.convT5_d(decoded))\n",
    "        print(decoded.size())                                   \n",
    "        decoded = F.relu(self.convT4_d(decoded))\n",
    "        print(decoded.size())\n",
    "        decoded = F.relu(self.convT3_d(decoded))\n",
    "        print(decoded.size())\n",
    "        decoded = F.relu(self.convT2_d(decoded))\n",
    "        print(decoded.size())\n",
    "        decoded = torch.sigmoid(self.convT1_d(decoded))\n",
    "        print(decoded.size())\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded new autoencoder\n",
      "torch.Size([8, 200, 4, 7])\n",
      "torch.Size([8, 200, 12, 17])\n",
      "torch.Size([8, 150, 55, 70])\n",
      "torch.Size([8, 128, 62, 77])\n",
      "torch.Size([8, 96, 126, 232])\n",
      "torch.Size([8, 64, 132, 238])\n",
      "torch.Size([8, 32, 269, 479])\n",
      "torch.Size([8, 16, 539, 959])\n",
      "torch.Size([8, 3, 1080, 1920])\n",
      "Train Epoche 1: 1% [>]\t accuracy: 0.000000, loss: 0.079411torch.Size([8, 200, 4, 7])\n",
      "torch.Size([8, 200, 12, 17])\n",
      "torch.Size([8, 150, 55, 70])\n",
      "torch.Size([8, 128, 62, 77])\n",
      "torch.Size([8, 96, 126, 232])\n",
      "torch.Size([8, 64, 132, 238])\n",
      "torch.Size([8, 32, 269, 479])\n",
      "torch.Size([8, 16, 539, 959])\n",
      "torch.Size([8, 3, 1080, 1920])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5488/1226555492.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dz_predict'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateSindy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dx_decode'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dz_predict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# total loss with SINDy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5488/167309269.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvT9_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\timok\\documents\\git_bachelor\\flowmodeling\\my_models\\neuralnetwork\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "encoder_path = 'results/Encoder_#epoch_v1.pt'\n",
    "decoder_path = 'results/Decoder_#epoch_v1.pt'\n",
    "if os.path.isfile(encoder_path):\n",
    "    encoder = torch.load(encoder_path)\n",
    "    print('loaded encoder', encoder_path)\n",
    "if os.path.isfile(decoder_path):\n",
    "    encoder = torch.load(decoder_path)\n",
    "    print('loaded decoder', decoder_path)\n",
    "else:\n",
    "    encoder = Encoder()\n",
    "    decoder = Decoder()\n",
    "    print('loaded new autoencoder')\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# optimization technique\n",
    "criterion = nn.MSELoss()\n",
    "# solution to add both encoder and decoder parameters is unknown?\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# to save network data\n",
    "network = {}\n",
    "\n",
    "# training\n",
    "outputs = []\n",
    "for epoch in range(params['number_epoch']):\n",
    "    for batch_id,img in enumerate(train_data):\n",
    "        encode = encoder(img)                 # encoded\n",
    "        recon = decoder(encode)               # reconstruction\n",
    "        # dx, dz is from current batch_id, x, z is from previous batch_id\n",
    "        if batch_id == 0:\n",
    "            network['x'] = img\n",
    "            network['z'] = encode\n",
    "            #network['x_decode'] = 0\n",
    "            network['dx'] = img\n",
    "            network['dz'] = encode\n",
    "            network['dx_decode'] = recon\n",
    "        else:\n",
    "            network['x'] = network['dx']\n",
    "            network['z'] = network['dz']\n",
    "            #network['x_decode'] = network['dx_decode']\n",
    "            network['dx'] = img\n",
    "            network['dz'] = encode\n",
    "            network['dx_decode'] = recon\n",
    "        \n",
    "        network['rec_loss'] = criterion(recon, img)\n",
    "        \n",
    "        # calculate SINDy\n",
    "        if batch_id >= 1:\n",
    "            network['dz_predict'] = calculateSindy(network, params)\n",
    "            network['dx_decode'] = decoder(network['dz_predict'])\n",
    "            total_loss = calculateLoss(network, params)            # total loss with SINDy\n",
    "        else:\n",
    "            total_loss = network['rec_loss']       \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accuracy = 0\n",
    "        printProgress(epoch, batch_id, accuracy, total_loss)\n",
    "        if batch_id == 2:\n",
    "            break\n",
    "    print('\\n')\n",
    "    outputs.append((epoch, img, recon))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prints\n",
    "torch.save(model, 'results/Autoencoder_2epoch_v1.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINDy prediction --> split up model that I can plug in z\n",
    "t = np.arange(0,20,.01)\n",
    "z_sim = sindy.sindy_simulate(network['z'], t, network['Xi'], params['poly_order'], params['include_sine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "print(network['Theta'].shape, network['Xi'].shape)\n",
    "#print(network['Xi'])\n",
    "\n",
    "print('reconstruction loss', categorie_loss[0],'\\n sindy x loss', categorie_loss[1], \n",
    "      '\\n sindy z loss', categorie_loss[2], '\\n Xi sparsity loss', categorie_loss[3])\n",
    "print(network['rec_loss'])\n",
    "\n",
    "for i in range(0, params['number_epoch']):\n",
    "    plt.figure()\n",
    "    realImg = outputs[i][1].permute(0,2,3,1).detach().numpy()\n",
    "    reconImg = outputs[i][2].permute(0,2,3,1).detach().numpy()\n",
    "    for i, item in enumerate(realImg):\n",
    "        if i >=4: break\n",
    "        plt.subplot(2,4,i+1)\n",
    "        plt.imshow(item)\n",
    "    for i, item in enumerate(reconImg):\n",
    "        if i >=4: break\n",
    "        plt.subplot(2,4,4+i+1)\n",
    "        plt.imshow(item)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
